{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wa-Kz67Nj3Ny"},"outputs":[],"source":["import os\n","\n","import csv\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import random\n","\n","import logging\n","import time\n","import tensorflow as tf\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.models import load_model\n","\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22264,"status":"ok","timestamp":1726573861445,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"onR8HRlXkBzg","outputId":"d08caa55-10b3-4e38-aec0-4df783af6a3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#file_path = '/content/drive/MyDrive/boilers_drive/normalized_df.csv'\n","train_csv_path = '/content/drive/MyDrive/boilers_drive/train_df.csv'\n","val_csv_path = '/content/drive/MyDrive/boilers_drive/val_df.csv'\n","test_csv_path = '/content/drive/MyDrive/boilers_drive/test_df.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFu6EVSBj3Nz"},"outputs":[],"source":["# Read the CSV file\n","#train_data = pd.read_csv(train_csv_path)\n","#val_data = pd.read_csv(val_csv_path)\n","test_data = pd.read_csv(test_csv_path)\n","\n","# Here we only need test data"]},{"cell_type":"code","source":["#print(train_data)\n","#print(val_data)\n","print(test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0B6s7jQwuBS6","executionInfo":{"status":"ok","timestamp":1726573877202,"user_tz":-180,"elapsed":444,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"40c4c72d-a63f-46f1-8918-9cf2364622ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                        time house_id  normalized_blr_mod_lvl  \\\n","0        2023-03-23 00:00:00  home101            2.975967e-09   \n","1        2023-03-23 00:01:00  home101            1.983978e-09   \n","2        2023-03-23 00:02:00  home101            1.322652e-09   \n","3        2023-03-23 00:03:00  home101            8.817681e-10   \n","4        2023-03-23 00:04:00  home101            5.878454e-10   \n","...                      ...      ...                     ...   \n","1542235  2023-04-30 23:55:00   home95            3.051793e-19   \n","1542236  2023-04-30 23:56:00   home95            2.034529e-19   \n","1542237  2023-04-30 23:57:00   home95            1.356353e-19   \n","1542238  2023-04-30 23:58:00   home95            9.042351e-20   \n","1542239  2023-04-30 23:59:00   home95            6.028234e-20   \n","\n","         normalized_absorption  normalized_insulation  normalized_t_r_set  \\\n","0                     0.278222               0.658129            0.711111   \n","1                     0.278081               0.659089            0.711111   \n","2                     0.277993               0.659089            0.711111   \n","3                     0.278489               0.659089            0.711111   \n","4                     0.277838               0.659089            0.711111   \n","...                        ...                    ...                 ...   \n","1542235               0.278356               0.506015            0.711111   \n","1542236               0.278065               0.501216            0.711111   \n","1542237               0.278071               0.502323            0.711111   \n","1542238               0.277830               0.501219            0.711111   \n","1542239               0.278096               0.499846            0.711111   \n","\n","         normalized_t_out  \n","0                0.345773  \n","1                0.344834  \n","2                0.344834  \n","3                0.344834  \n","4                0.344834  \n","...                   ...  \n","1542235          0.496677  \n","1542236          0.499609  \n","1542237          0.499902  \n","1542238          0.499071  \n","1542239          0.500413  \n","\n","[1542240 rows x 7 columns]\n"]}]},{"cell_type":"code","source":["# load pre-prepared random order of houses\n","random_order_houses = pd.read_csv('/content/drive/MyDrive/boilers_drive/random_order_houses.csv')"],"metadata":{"id":"ImLq7Is9uEHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Different houses in data\n","houses = random_order_houses['house_id'].unique().tolist()\n","print(\"Different houses in data:\")\n","print(houses)\n","print(\"Number of different houses:\")\n","print(len(houses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5NZAwtpQuGfp","executionInfo":{"status":"ok","timestamp":1726573887756,"user_tz":-180,"elapsed":448,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"8810a63b-850a-49d8-acad-03b66672143c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Different houses in data:\n","['home9', 'home114', 'home5', 'home89', 'home17', 'home63', 'home2', 'home101', 'home14', 'home95', 'home111', 'home67', 'home77', 'home43', 'home86', 'home90', 'home47', 'home110', 'home93', 'home53', 'home34', 'home51', 'home106', 'home46', 'home79', 'home55', 'home65', 'home13']\n","Number of different houses:\n","28\n"]}]},{"cell_type":"markdown","source":["**Defining the transformer model for proper loading**"],"metadata":{"id":"euaTDIO82sKb"}},{"cell_type":"code","source":["# !! Defining the transformer !!\n","\n","# positional encoding layer\n","def positional_encoding(length, depth):\n","  depth = depth/2\n","\n","  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","  angle_rates = 1 / (10000**depths)         # (1, depth)\n","  angle_rads = positions * angle_rates      # (pos, depth)\n","\n","  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","# positional embedding layer\n","class PositionalEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, vocab_size, d_model):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.embedding = tf.keras.layers.Dense(d_model)  # Project input to d_model dimension\n","    #self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n","    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n","\n","  def compute_mask(self, *args, **kwargs):\n","    return self.embedding.compute_mask(*args, **kwargs)\n","\n","  def call(self, x):\n","    length = tf.shape(x)[1]\n","    x = self.embedding(x)\n","    # This factor sets the relative scale of the embedding and positonal_encoding.\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x = x + self.pos_encoding[tf.newaxis, :length, :]\n","    return x\n","\n","# Attention layers\n","# These are all identical except for how the attention is configured\n","\n","# base attention layer\n","class BaseAttention(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","    self.add = tf.keras.layers.Add()\n","\n","\n","# cross attention layer\n","# (at the center of the Transformer is the cross-attention layer, it connects the encoder and decoder)\n","class CrossAttention(BaseAttention):\n","  def call(self, x, context):\n","    attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n","    # Cache the attention scores for plotting later.\n","    self.last_attn_scores = attn_scores\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","\n","# global self attention layer\n","# (it is responsible for processing the context sequence, and propagating information along its length)\n","class GlobalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","\n","# causal self attention layer\n","# (it does a similar job as the global self attention layer, for the output sequence)\n","class CausalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","\n","# feed forward network\n","# (the transformer also includes this point-wise feed-forward network in both the encoder and decoder)\n","class FeedForward(tf.keras.layers.Layer):\n","  def __init__(self, d_model, dff, dropout_rate=0.1):\n","    super().__init__()\n","    self.seq = tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),\n","      tf.keras.layers.Dense(d_model),\n","      tf.keras.layers.Dropout(dropout_rate)\n","    ])\n","    self.add = tf.keras.layers.Add()\n","    self.layer_norm = tf.keras.layers.LayerNormalization()\n","\n","  def call(self, x):\n","    x = self.add([x, self.seq(x)])\n","    x = self.layer_norm(x)\n","    return x\n","\n","\n","# encoder layer\n","# (the encoder contains a stack of N encoder layers. Where each EncoderLayer contains\n","#   a GlobalSelfAttention and FeedForward layer)\n","class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x):\n","    x = self.self_attention(x)\n","    x = self.ffn(x)\n","    return x\n","\n","\n","# The Encoder\n","class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","\n","    self.enc_layers = [\n","        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","\n","  def call(self, x):\n","    # `x` is token-IDs shape: (batch, seq_len)\n","    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    # Add dropout.\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x)\n","\n","    return x  # Shape `(batch_size, seq_len, d_model)`.\n","\n","\n","# decoder layer\n","# (the decoder's stack is slightly more complex, with each DecoderLayer containing\n","#   a CausalSelfAttention, a CrossAttention, and a FeedForward layer)\n","class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.causal_self_attention = CausalSelfAttention(\n","        num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","\n","    self.cross_attention = CrossAttention(\n","        num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x, context):\n","    x = self.causal_self_attention(x=x)\n","    x = self.cross_attention(x=x, context=context)\n","    # Cache the last attention scores for plotting later\n","    self.last_attn_scores = self.cross_attention.last_attn_scores\n","    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    return x\n","\n","\n","# The Decoder\n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","    self.dec_layers = [\n","        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","\n","    self.last_attn_scores = None\n","\n","\n","  def call(self, x, context):\n","    # `x` is token-IDs shape (batch, target_seq_len)\n","    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x  = self.dec_layers[i](x, context)\n","\n","    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n","    # The shape of x is (batch_size, target_seq_len, d_model).\n","    return x\n","\n","\n","# The Transformer\n","@tf.keras.utils.register_keras_serializable()\n","class Transformer(tf.keras.Model):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff,\n","               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.input_proj = tf.keras.layers.Dense(d_model)  # Project input to the model dimension\n","\n","    self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n","\n","    self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n","\n","    #self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    self.final_layer = tf.keras.layers.Dense(1)\n","\n","\n","  def call(self, inputs):  # Expecting only one input\n","    x = inputs\n","    #x = self.input_proj(x) # because we get n values for every 1 output value\n","    context = self.encoder(x)\n","    x = self.decoder(x, context)\n","    logits = self.final_layer(x)\n","    try:\n","      del logits._keras_mask\n","    except AttributeError:\n","      pass\n","    return logits\n"],"metadata":{"id":"9TcajKwb20Yo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nt8LA8qyj3N3"},"source":["**Preparing the data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byevh-BWj3N3"},"outputs":[],"source":["# the categories for prediction\n","final_category = 'blr_mod_lvl'\n","prediction_categories = ['blr_mod_lvl', 'absorption', 'insulation', 't_r_set', 't_out']\n","normalized_categories = ['normalized_blr_mod_lvl', 'normalized_absorption', 'normalized_insulation', 'normalized_t_r_set', 'normalized_t_out']"]},{"cell_type":"code","source":["# Getting the trained model from saved file\n","\n","def load_transformer_model(input_categories, num_of_houses):\n","    categories_used = '_'.join(input_categories)\n","    houses_used = str(num_of_houses)\n","\n","    path_to_file = '/content/drive/Othercomputers/My_Laptop/code/models/'\n","    model_name = 'transfomrer_'+categories_used+\"_houses_\"+houses_used\n","    # !!! due to typing error the names have 'transfomrer' instead of 'transformer' !!\n","    path_to_saved_model = path_to_file + model_name + '.keras'\n","    path_to_history = path_to_file + model_name + '_history.pkl'\n","\n","    # Loading from store\n","    model = tf.keras.models.load_model(path_to_saved_model, custom_objects={'Transformer': Transformer})\n","    with open(path_to_history, 'rb') as f:\n","        history = pickle.load(f)\n","\n","    return model, history\n","\n","\n","def load_lstm_model(input_categories, num_of_houses):\n","    categories_used = '_'.join(input_categories)\n","    houses_used = str(num_of_houses)\n","\n","    path_to_file = '/content/drive/Othercomputers/My_Laptop/code/models/'\n","    model_name = 'lstm_'+categories_used+\"_houses_\"+houses_used\n","\n","    path_to_saved_model = path_to_file + model_name + '.keras'\n","    path_to_history = path_to_file + model_name + '_history.pkl'\n","\n","    # Loading from store\n","    model = tf.keras.models.load_model(path_to_saved_model)\n","    with open(path_to_history, 'rb') as f:\n","        history = pickle.load(f)\n","\n","    return model, history\n","\n","\n","model, history = load_transformer_model(['blr_mod_lvl'], 1)\n","\n","print(model.summary())\n","print(history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"id":"KIiaTkymd932","executionInfo":{"status":"error","timestamp":1726578510714,"user_tz":-180,"elapsed":451,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"c1fbe012-ea73-4d3e-a279-81a1774441e0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"<class '__main__.Transformer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': None, 'class_name': 'Transformer', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'Transformer', 'build_config': {'input_shape': [None, 1440, 1]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.0010000000474974513, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': [{'module': 'keras.metrics', 'class_name': 'MeanAbsoluteError', 'config': {'name': 'mean_absolute_error', 'dtype': 'float32'}, 'registered_name': None}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': True}}.\n\nException encountered: Transformer.__init__() got an unexpected keyword argument 'trainable'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-57-0756b037e4f5>\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() got an unexpected keyword argument 'trainable'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-e30071b4065a>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blr_mod_lvl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-e30071b4065a>\u001b[0m in \u001b[0;36mload_transformer_model\u001b[0;34m(input_categories, num_of_houses)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Loading from store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_saved_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Transformer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    235\u001b[0m             )\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: <class '__main__.Transformer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': None, 'class_name': 'Transformer', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'Transformer', 'build_config': {'input_shape': [None, 1440, 1]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.0010000000474974513, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': [{'module': 'keras.metrics', 'class_name': 'MeanAbsoluteError', 'config': {'name': 'mean_absolute_error', 'dtype': 'float32'}, 'registered_name': None}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': True}}.\n\nException encountered: Transformer.__init__() got an unexpected keyword argument 'trainable'"]}]},{"cell_type":"code","source":["model, history = load_transformer_model(['blr_mod_lvl'], 1)\n","\n","print(model.summary())\n","print(history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":894},"id":"_0UK9S2Ht_dx","executionInfo":{"status":"error","timestamp":1726575154023,"user_tz":-180,"elapsed":455,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"01cc2e0b-7681-4763-bc9e-ffb10a8e4eff"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"<class '__main__.Transformer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': None, 'class_name': 'Transformer', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'Transformer', 'build_config': {'input_shape': [None, 1440, 1]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.0010000000474974513, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': [{'module': 'keras.metrics', 'class_name': 'MeanAbsoluteError', 'config': {'name': 'mean_absolute_error', 'dtype': 'float32'}, 'registered_name': None}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': True}}.\n\nException encountered: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.Transformer'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of Transformer from its config.\n\nReceived config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n\nError encountered during deserialization: Transformer.__init__() got an unexpected keyword argument 'trainable'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() got an unexpected keyword argument 'trainable'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0;34m\"Unable to revive model from config. When overriding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.Transformer'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of Transformer from its config.\n\nReceived config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n\nError encountered during deserialization: Transformer.__init__() got an unexpected keyword argument 'trainable'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-e350b68545d5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blr_mod_lvl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-f5cc568eacc5>\u001b[0m in \u001b[0;36mload_transformer_model\u001b[0;34m(input_categories, num_of_houses)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     model = tf.keras.models.load_model(path_to_saved_model, custom_objects={'Transformer': Transformer, \n\u001b[0m\u001b[1;32m     22\u001b[0m                                                                            \u001b[0;34m'PositionalEmbedding'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPositionalEmbedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                            \u001b[0;34m'GlobalSelfAttention'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGlobalSelfAttention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    235\u001b[0m             )\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: <class '__main__.Transformer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': None, 'class_name': 'Transformer', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'Transformer', 'build_config': {'input_shape': [None, 1440, 1]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.0010000000474974513, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': [{'module': 'keras.metrics', 'class_name': 'MeanAbsoluteError', 'config': {'name': 'mean_absolute_error', 'dtype': 'float32'}, 'registered_name': None}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': True}}.\n\nException encountered: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.Transformer'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of Transformer from its config.\n\nReceived config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n\nError encountered during deserialization: Transformer.__init__() got an unexpected keyword argument 'trainable'"]}]},{"cell_type":"code","source":["# Plotting the training and validation loss and MAE\n","def training_plots(history):\n","    # Plotting the training and validation loss and MAE\n","    plt.figure(figsize=(14, 5))\n","\n","    # Plot training & validation loss values\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Model Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss (MSE)')\n","    plt.legend(loc='upper right')\n","\n","    # Plot training & validation MAE values\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n","    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n","    plt.title('Model Mean Absolute Error')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('MAE')\n","    plt.legend(loc='upper right')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"v75nruiWxP-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_plots(history)"],"metadata":{"id":"kd2j5d2ixRqh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The necessary functions to get the correct test dataset for predictions !!\n","\n","# creating sub-lists, each with the data of one day\n","# the function that gets category data of a house (ordered by date) and separates by day\n","def separate_into_days(data_list, minutes_per_day=1440):\n","    # number of days\n","    num_days = len(data_list) // minutes_per_day\n","    # the data into a list of sub-lists, each containing one day's data\n","    separated_data = [\n","        data_list[i * minutes_per_day:(i + 1) * minutes_per_day]\n","        for i in range(num_days)\n","    ]\n","    return separated_data\n","\n","\n","# function to \"combine\" values of categories and separate into sub-lists based on days\n","def combine_categories(dataset, categories_list):\n","    # Combine specified categories into lists\n","    combined_elements = dataset[categories_list].apply(lambda row: row.tolist(), axis=1)\n","    return combined_elements.tolist()\n","\n","\n","def prepare_data_2(house_data, input_categories, output_category, minutes_per_day=1440):\n","    combined_input_data = combine_categories(house_data, input_categories)\n","    separated_input_data = separate_into_days(combined_input_data, minutes_per_day)\n","    output = house_data[output_category].values\n","    separated_output = separate_into_days(output, minutes_per_day)\n","    # [:, :-1] and [:, 1:] for 2-d arrays\n","    # [:-1],  and [1:] for 1-d arrays (or lists)\n","    # all except last day are inputs (for prediction)\n","    input_data = separated_input_data[:-1]\n","    # all except first day are the corresponding outputs (from prediction)\n","    output_data = separated_output[1:]\n","    return input_data, output_data\n","\n","\n","# function for getting input and target data\n","def input_target_split (data, input_categories, num_of_houses):\n","    filtered_data = data[['house_id', 'time', 'normalized_blr_mod_lvl', 'normalized_absorption', 'normalized_insulation', 'normalized_t_r_set', 'normalized_t_out']].copy()\n","    input_chosen_categories = []\n","    for cat in input_categories:\n","        input_chosen_categories.append('normalized_'+cat)\n","\n","    # We have 1440 minutes per day\n","    minutes_per_day = 1440\n","\n","    # Initialize lists to store input and target pairs\n","    input_data_list, target_data_list = [], []\n","\n","    for house_id in houses[:num_of_houses]:\n","        house_data = filtered_data[filtered_data['house_id'] == house_id]\n","        house_data = house_data.sort_values(by='time')\n","        input_data, target_data = prepare_data_2(house_data, input_chosen_categories, 'normalized_blr_mod_lvl')\n","        input_data_list.append(input_data)\n","        target_data_list.append(target_data)\n","\n","    # Combine all houses' data\n","    input_data = np.concatenate(input_data_list, axis=0)\n","    target_data = np.concatenate(target_data_list, axis=0)\n","\n","    return input_data, target_data\n","\n","\n","def get_split_test_data(test_data, input_categories, num_of_houses):\n","    input_test, target_test = input_target_split(test_data, input_categories, num_of_houses)\n","    return input_test, target_test\n","\n","\n","def get_test_datasets(input_test, target_test):\n","    batch_size = 8 # Reduce the batch size, in case it helps !\n","\n","    test_dataset = tf.data.Dataset.from_tensor_slices((input_test, target_test))\n","    test_dataset = test_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    # The validation_split argument is designed to work with NumPy arrays or TensorFlow tensors,\n","    # where it can easily split the data based on a fraction.\n","    # It doesn't directly work with TensorFlow datasets because they handle data differently.\n","    return test_dataset\n","\n","\n","def print_test_shapes(input_test, target_test):\n","    print(input_test.shape)\n","    print(target_test.shape)\n","    num_of_categories = input_test.shape[-1]\n","    print(\"Number of categories for prediction: \"+str(num_of_categories))\n","    return num_of_categories"],"metadata":{"id":"XSKpHjdHu06g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the correct test dataset\n","def get_test_dataset(test_data, input_categories, num_of_houses):\n","    print(\"splitting data\")\n","    input_test, target_test = get_split_test_data(test_data, input_categories, num_of_houses)\n","    print(\"getting datasets\")\n","    test_dataset = get_test_datasets(input_test, target_test)\n","    print(\"printing shapes and getting num_of_categories\")\n","    num_of_categories = print_test_shapes(input_test, target_test)\n","    return test_dataset"],"metadata":{"id":"BqqCKU12wKNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test dataset\n","test_results = model.evaluate(test_dataset, return_dict=True)\n","#print(f\"Test Loss (MSE): {test_results['loss']}, Test MAE: {test_results['mean_absolute_error']}\")\n","\n","# model.predict() !!! (not model.evaluate)"],"metadata":{"id":"d7GAXgAIiX4P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721381728829,"user_tz":-180,"elapsed":32921,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"0f5c09e3-4629-42cf-ee0a-54ae17d38d9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["148/148 [==============================] - 33s 222ms/step - loss: 0.0068 - mean_absolute_error: 0.0314\n"]}]},{"cell_type":"code","source":["test_predictions = model.predict(test_dataset)"],"metadata":{"id":"_M9GC24xHiLN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721381771342,"user_tz":-180,"elapsed":42526,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"954e05fc-5eea-4ef1-ee74-5a57e8f5d405"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["148/148 [==============================] - 34s 219ms/step\n"]}]},{"cell_type":"code","source":["#print(test_predictions.shape)\n","#flattened_predictions = test_predictions.reshape(-1)\n","#print(flattened_predictions.shape)\n","#print(flattened_predictions)\n","#de_scaled_predictions = de_scale(flattened_predictions.reshape(-1, 1), 'blr_mod_lvl')\n","#print(de_scaled_predictions)\n","\n","# we get the predictions that correspond to the target_test\n","\n","#print(target_test.shape)\n","#flattened_target = target_test.reshape(-1)\n","#print(flattened_target)\n","#de_scaled_target = de_scale(flattened_target.reshape(-1, 1), 'blr_mod_lvl')\n","#print(de_scaled_target)"],"metadata":{"id":"BmuAFgkSOdBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_test_values(test_values):\n","  flattened_test_values = test_values.reshape(-1)\n","  de_scaled_test_values = de_scale(flattened_test_values.reshape(-1, 1), 'blr_mod_lvl')\n","  final_test_values = de_scaled_test_values.reshape(-1)\n","  return final_test_values"],"metadata":{"id":"RDimzVECDreC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function which randomly selects one day from the original prediction results\n","def get_random_day(predictions, target):\n","  random_day_index = random.randint(0, predictions.shape[0] - 1)\n","  selected_day_prediction = predictions[random_day_index]\n","  selected_day_target = target[random_day_index]\n","  selected_day_prediction_final = get_test_values(selected_day_prediction)\n","  selected_day_target_final = get_test_values(selected_day_target)\n","  return selected_day_prediction_final, selected_day_target_final"],"metadata":{"id":"PBa8Z-Huezom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_day_prediction, random_day_target = get_random_day(test_predictions, target_test)\n","print(\"Random Day Prediction:\", random_day_prediction)\n","print(\"Random Day Target:\", random_day_target)"],"metadata":{"id":"SV637FqcfZM5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721381771344,"user_tz":-180,"elapsed":16,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"8234b03a-9575-4eb2-8d66-4bdbef64061e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Day Prediction: [ 5.0322704  4.8768334  4.7818475 ... -1.0015002 -1.141188  -1.2381945]\n","Random Day Target: [19.89303692 18.42637647 14.67591764 ...  1.84059946  1.2270663\n","  0.8180442 ]\n"]}]},{"cell_type":"code","source":["# function that gives us the error metrics for given pair target, prediction\n","def get_error_metrics(target, prediction):\n","  # error values\n","  error = [t - p for t, p in zip(target, prediction)]\n","  AE = [abs(e) for e in error] # Absolute Error\n","  SE = [e ** 2 for e in error] # Squared Error\n","  APE = [abs((t - p) / t) for t, p in zip(target, prediction)] # Absolute Percentage Error\n","\n","  # error metrics\n","  # Mean Absolute Error, gives magnitude of errors without caring for direction\n","  MAE = np.mean(AE)\n","  # Mean Squared Error, gives higher weight for larger errors\n","  MSE = np.mean(SE)\n","  # Root Mean Squared Error,  it is in the same units as the target variable\n","  RMSE = np.sqrt(MSE)\n","  # Mean Absolute Percentage Error, provides a perspective on the size of the error relative to the target values (given as percentage %)\n","  MAPE = np.mean(APE) * 100\n","  # R-Squared, statistical measure that represents the proportion of the variance for the target variable that's explained by the model\n","  # provides an indication of the goodness of fit\n","  mean_target = np.mean(target)\n","  diff = [t - mean_target for t in target]\n","  denominator = [d ** 2 for d in diff]\n","  R2 = 1 - (sum(SE) / sum(denominator))\n","\n","  # minimum and maximum errors and values\n","  minimum_error = np.min(error)\n","  maximum_error = np.max(error)\n","  min_pred = np.min(prediction)\n","  max_pred = np.max(prediction)\n","  min_target = np.min(target)\n","  max_target = np.max(target)\n","\n","  errors = pd.DataFrame({\n","      'blr_mod_lvl': target,\n","      'prediction': prediction,\n","      'error': error,\n","      'AE': AE,\n","      'SE': SE,\n","      'APE': APE\n","  })\n","\n","  errors['MAE'] = MAE\n","  errors['MSE'] = MSE\n","  errors['RMSE'] = RMSE\n","  errors['MAPE'] = MAPE\n","  errors['R2'] = R2\n","  errors['minimum_error'] = minimum_error\n","  errors['maximum_error'] = maximum_error\n","  errors['minimum_prediction'] = min_pred\n","  errors['maximum_prediction'] = max_pred\n","  errors['minimum_target'] = min_target\n","  errors['maximum_target'] = max_target\n","\n","  return errors\n"],"metadata":{"id":"kt3OiJd9RtV_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target = get_test_values(target_test)\n","prediction = get_test_values(test_predictions)\n","\n","errors = get_error_metrics(target, prediction)\n","\n","print(\"\\n Table with errors\")\n","print(errors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncVXsE5vdg2X","executionInfo":{"status":"ok","timestamp":1721381858577,"user_tz":-180,"elapsed":4389,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"629589ec-e532-40b8-9aa8-85b2ec83d7ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-38-470568afdeb7>:7: RuntimeWarning: divide by zero encountered in scalar divide\n","  APE = [abs((t - p) / t) for t, p in zip(target, prediction)] # Absolute Percentage Error\n","<ipython-input-38-470568afdeb7>:7: RuntimeWarning: overflow encountered in scalar divide\n","  APE = [abs((t - p) / t) for t, p in zip(target, prediction)] # Absolute Percentage Error\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Table with errors\n","          blr_mod_lvl  prediction     error        AE        SE           APE  \\\n","0        2.048427e-08    0.171338 -0.171338  0.171338  0.029357  8.364362e+06   \n","1        1.365618e-08    0.158856 -0.158856  0.158856  0.025235  1.163255e+07   \n","2        9.104121e-09    0.166520 -0.166520  0.166520  0.027729  1.829066e+07   \n","3        6.069414e-09    0.190216 -0.190216  0.190216  0.036182  3.134002e+07   \n","4        4.046276e-09    0.202335 -0.202335  0.202335  0.040939  5.000519e+07   \n","...               ...         ...       ...       ...       ...           ...   \n","1702075  6.794076e-19    0.520067 -0.520067  0.520067  0.270470  7.654714e+17   \n","1702076  4.529384e-19    0.500196 -0.500196  0.500196  0.250196  1.104336e+18   \n","1702077  3.019589e-19    0.486260 -0.486260  0.486260  0.236449  1.610352e+18   \n","1702078  2.013060e-19    0.461625 -0.461625  0.461625  0.213097  2.293150e+18   \n","1702079  1.342040e-19    0.394146 -0.394146  0.394146  0.155351  2.936917e+18   \n","\n","              MAE        MSE      RMSE  MAPE        R2  minimum_error  \\\n","0        2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","1        2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","2        2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","3        2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","4        2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","...           ...        ...       ...   ...       ...            ...   \n","1702075  2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","1702076  2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","1702077  2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","1702078  2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","1702079  2.648631  48.233013  6.944999   inf  0.395029     -58.738518   \n","\n","         maximum_error  minimum_prediction  maximum_prediction  \\\n","0            72.462446            -3.68986           63.359993   \n","1            72.462446            -3.68986           63.359993   \n","2            72.462446            -3.68986           63.359993   \n","3            72.462446            -3.68986           63.359993   \n","4            72.462446            -3.68986           63.359993   \n","...                ...                 ...                 ...   \n","1702075      72.462446            -3.68986           63.359993   \n","1702076      72.462446            -3.68986           63.359993   \n","1702077      72.462446            -3.68986           63.359993   \n","1702078      72.462446            -3.68986           63.359993   \n","1702079      72.462446            -3.68986           63.359993   \n","\n","         minimum_target  maximum_target  \n","0                   0.0       73.164528  \n","1                   0.0       73.164528  \n","2                   0.0       73.164528  \n","3                   0.0       73.164528  \n","4                   0.0       73.164528  \n","...                 ...             ...  \n","1702075             0.0       73.164528  \n","1702076             0.0       73.164528  \n","1702077             0.0       73.164528  \n","1702078             0.0       73.164528  \n","1702079             0.0       73.164528  \n","\n","[1702080 rows x 17 columns]\n"]}]},{"cell_type":"code","source":["# Now the errors, error metrics for the random day\n","errors_day = get_error_metrics(random_day_target, random_day_prediction)\n","\n","print(\"\\n Table with errors for random day\")\n","print(errors_day)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dl_GSAixFA30","executionInfo":{"status":"ok","timestamp":1721381861959,"user_tz":-180,"elapsed":360,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"586293c5-f166-4ddd-9bb6-1538098a30de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Table with errors for random day\n","      blr_mod_lvl  prediction      error         AE          SE       APE  \\\n","0       19.893037    5.032270  14.860766  14.860766  220.842381  0.747034   \n","1       18.426376    4.876833  13.549543  13.549543  183.590116  0.735334   \n","2       14.675918    4.781847   9.894070   9.894070   97.892624  0.674170   \n","3       10.114501    4.724797   5.389704   5.389704   29.048908  0.532869   \n","4        6.743000    4.799429   1.943571   1.943571    3.777470  0.288235   \n","...           ...         ...        ...        ...         ...       ...   \n","1435     4.141349   -0.665097   4.806446   4.806446   23.101924  1.160599   \n","1436     2.760899   -0.907946   3.668845   3.668845   13.460422  1.328859   \n","1437     1.840599   -1.001500   2.842100   2.842100    8.077531  1.544116   \n","1438     1.227066   -1.141188   2.368254   2.368254    5.608629  1.930013   \n","1439     0.818044   -1.238194   2.056239   2.056239    4.228117  2.513603   \n","\n","           MAE       MSE       RMSE  MAPE        R2  minimum_error  \\\n","0     6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","1     6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","2     6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","3     6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","4     6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","...        ...       ...        ...   ...       ...            ...   \n","1435  6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","1436  6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","1437  6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","1438  6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","1439  6.150582  139.0854  11.793447   inf  0.440435     -21.941742   \n","\n","      maximum_error  minimum_prediction  maximum_prediction  minimum_target  \\\n","0         57.733397           -2.928067           50.770027             0.0   \n","1         57.733397           -2.928067           50.770027             0.0   \n","2         57.733397           -2.928067           50.770027             0.0   \n","3         57.733397           -2.928067           50.770027             0.0   \n","4         57.733397           -2.928067           50.770027             0.0   \n","...             ...                 ...                 ...             ...   \n","1435      57.733397           -2.928067           50.770027             0.0   \n","1436      57.733397           -2.928067           50.770027             0.0   \n","1437      57.733397           -2.928067           50.770027             0.0   \n","1438      57.733397           -2.928067           50.770027             0.0   \n","1439      57.733397           -2.928067           50.770027             0.0   \n","\n","      maximum_target  \n","0          64.286066  \n","1          64.286066  \n","2          64.286066  \n","3          64.286066  \n","4          64.286066  \n","...              ...  \n","1435       64.286066  \n","1436       64.286066  \n","1437       64.286066  \n","1438       64.286066  \n","1439       64.286066  \n","\n","[1440 rows x 17 columns]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-38-470568afdeb7>:7: RuntimeWarning: divide by zero encountered in scalar divide\n","  APE = [abs((t - p) / t) for t, p in zip(target, prediction)] # Absolute Percentage Error\n"]}]},{"cell_type":"code","source":["# get plots with the error metrics\n","def plot_error_metrics(errors):\n","    plt.figure(figsize=(14, 10))\n","    # we will make n plots\n","    ploting_items = errors\n","    plot_list = [['blr_mod_lvl', 'prediction', 'minimum_prediction', 'maximum_prediction', 'minimum_target', 'maximum_target'],\n","                  ['error', 'minimum_error', 'maximum_error'], ['AE', 'MAE', 'RMSE'], ['SE', 'MSE']]\n","    n = len(plot_list)\n","    for i in range(n):\n","        to_plot = plot_list[i]\n","        plt.subplot(1, n+1, i)\n","        for j in range(1, len(to_plot)+1):\n","            to_plot_name = to_plot[j-1]\n","            plt.plot(ploting_items[to_plot_name], label=to_plot_name)\n","        #plt.title()\n","        plt.xlabel('Time')\n","        #plt.ylabel()\n","        plt.legend(loc='upper right')"],"metadata":{"id":"RrspSEgYd5ci"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}