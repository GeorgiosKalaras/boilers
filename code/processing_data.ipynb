{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11156,"status":"ok","timestamp":1718701437254,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"wa-Kz67Nj3Ny"},"outputs":[],"source":["import os\n","\n","import csv\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.ensemble import IsolationForest\n","from sklearn.svm import OneClassSVM\n","\n","import logging\n","import time\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24936,"status":"ok","timestamp":1718701462187,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"onR8HRlXkBzg","outputId":"5bc2c20c-0a6a-42a3-a07b-1f23f3bc8c06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/boilers_drive/normalized_df.csv'"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17847,"status":"ok","timestamp":1718701480028,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"EFu6EVSBj3Nz","outputId":"d8208af8-9181-4d36-fcc1-e7af062b5d04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imported data:\n","                        time    blr_mod_lvl  absorption  insulation  t_r_set  \\\n","0        2022-10-01 00:00:00   0.000000e+00    0.503910   -7.457292     15.0   \n","1        2022-10-01 00:01:00   0.000000e+00    0.503910   -7.455208     15.0   \n","2        2022-10-01 00:02:00   0.000000e+00    0.518558   -7.487500     15.0   \n","3        2022-10-01 00:03:00   0.000000e+00    0.616207   -7.426042     15.0   \n","4        2022-10-01 00:04:00   0.000000e+00    0.699210   -7.425000     15.0   \n","...                      ...            ...         ...         ...      ...   \n","7331983  2023-04-30 23:55:00  2.871866e-119   -0.296880   -1.604167     17.0   \n","7331984  2023-04-30 23:56:00  1.914578e-119   -0.296880   -1.614583     17.0   \n","7331985  2023-04-30 23:57:00  1.276385e-119   -0.296880   -1.572917     17.0   \n","7331986  2023-04-30 23:58:00  8.509234e-120   -0.296880   -1.511458     17.0   \n","7331987  2023-04-30 23:59:00  5.672823e-120   -0.296880   -1.432203     17.0   \n","\n","             t_out house_id  \n","0        22.342708    home2  \n","1        22.344792    home2  \n","2        22.312500    home2  \n","3        22.373958    home2  \n","4        22.375000    home2  \n","...            ...      ...  \n","7331983  17.395833  home114  \n","7331984  17.385417  home114  \n","7331985  17.427083  home114  \n","7331986  17.488542  home114  \n","7331987  17.567797  home114  \n","\n","[7331988 rows x 7 columns]\n"]}],"source":["# Specify the file path of your CSV file\n","#file_path = 'normalized_df.csv'\n","\n","# the current script directory\n","# the current working directory\n","#current_dir = os.getcwd()\n","\n","# the path to the CSV file in the parent directory\n","#parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n","#file_path = os.path.join(parent_dir, 'normalized_df.csv')\n","\n","# Read the CSV file\n","data = pd.read_csv(file_path)\n","\n","print(\"Imported data:\")\n","print(data)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577,"status":"ok","timestamp":1718701480594,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"EN_3lAXbj3N0","outputId":"96ece8b7-a630-4059-be7d-4906681f75c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Different houses in data:\n","['home2' 'home9' 'home13' 'home14' 'home34' 'home46' 'home55' 'home67'\n"," 'home86' 'home93' 'home101' 'home106' 'home110' 'home43' 'home63'\n"," 'home53' 'home79' 'home90' 'home95' 'home5' 'home17' 'home47' 'home51'\n"," 'home65' 'home77' 'home89' 'home111' 'home114']\n","Number of different houses:\n","28\n"]}],"source":["# Different houses in data\n","houses = data['house_id'].unique()\n","print(\"Different houses in data:\")\n","print(houses)\n","print(\"Number of different houses:\")\n","print(len(houses))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":17256,"status":"ok","timestamp":1718701497848,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"P033elOTj3N0"},"outputs":[],"source":["house_datasets = {}\n","for house in houses:\n","    house_datasets[house] = data[data['house_id'] == house]\n","\n","#print(house_datasets)"]},{"cell_type":"markdown","metadata":{"id":"fwbpJ2Qmj3N1"},"source":["**Δημιουργία του encoder decoder transformer**"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718701497848,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"x01wafJKj3N1"},"outputs":[],"source":["# positional encoding layer\n","def positional_encoding(length, depth):\n","  depth = depth/2\n","\n","  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","  angle_rates = 1 / (10000**depths)         # (1, depth)\n","  angle_rads = positions * angle_rates      # (pos, depth)\n","\n","  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","# positional embedding layer\n","class PositionalEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, vocab_size, d_model):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n","    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n","\n","  def compute_mask(self, *args, **kwargs):\n","    return self.embedding.compute_mask(*args, **kwargs)\n","\n","  def call(self, x):\n","    length = tf.shape(x)[1]\n","    x = self.embedding(x)\n","    # This factor sets the relative scale of the embedding and positonal_encoding.\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x = x + self.pos_encoding[tf.newaxis, :length, :]\n","    return x"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718701497849,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"ZRjqbnDpj3N2"},"outputs":[],"source":["# Attention layers\n","# These are all identical except for how the attention is configured\n","\n","# base attention layer\n","class BaseAttention(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","    self.add = tf.keras.layers.Add()\n","\n","\n","# cross attention layer\n","# (at the center of the Transformer is the cross-attention layer, it connects the encoder and decoder)\n","class CrossAttention(BaseAttention):\n","  def call(self, x, context):\n","    attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n","    # Cache the attention scores for plotting later.\n","    self.last_attn_scores = attn_scores\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","\n","# global self attention layer\n","# (it is responsible for processing the context sequence, and propagating information along its length)\n","class GlobalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","\n","# causal self attention layer\n","# (it does a similar job as the global self attention layer, for the output sequence)\n","class CausalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718701497849,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"mQypWHIuj3N2"},"outputs":[],"source":["# feed forward network\n","# (the transformer also includes this point-wise feed-forward network in both the encoder and decoder)\n","class FeedForward(tf.keras.layers.Layer):\n","  def __init__(self, d_model, dff, dropout_rate=0.1):\n","    super().__init__()\n","    self.seq = tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),\n","      tf.keras.layers.Dense(d_model),\n","      tf.keras.layers.Dropout(dropout_rate)\n","    ])\n","    self.add = tf.keras.layers.Add()\n","    self.layer_norm = tf.keras.layers.LayerNormalization()\n","\n","  def call(self, x):\n","    x = self.add([x, self.seq(x)])\n","    x = self.layer_norm(x)\n","    return x"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718701497849,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"5SpzNhEXj3N2"},"outputs":[],"source":["# encoder layer\n","# (the encoder contains a stack of N encoder layers. Where each EncoderLayer contains\n","#   a GlobalSelfAttention and FeedForward layer)\n","class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x):\n","    x = self.self_attention(x)\n","    x = self.ffn(x)\n","    return x"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718701497849,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"50K_L14Vj3N2"},"outputs":[],"source":["# The Encoder\n","class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","\n","    self.enc_layers = [\n","        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","\n","  def call(self, x):\n","    # `x` is token-IDs shape: (batch, seq_len)\n","    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    # Add dropout.\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x)\n","\n","    return x  # Shape `(batch_size, seq_len, d_model)`."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718701497849,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"lbvX_YJqj3N2"},"outputs":[],"source":["# decoder layer\n","# (the decoder's stack is slightly more complex, with each DecoderLayer containing\n","#   a CausalSelfAttention, a CrossAttention, and a FeedForward layer)\n","class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.causal_self_attention = CausalSelfAttention(\n","        num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","\n","    self.cross_attention = CrossAttention(\n","        num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x, context):\n","    x = self.causal_self_attention(x=x)\n","    x = self.cross_attention(x=x, context=context)\n","    # Cache the last attention scores for plotting later\n","    self.last_attn_scores = self.cross_attention.last_attn_scores\n","    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    return x"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718701497849,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"vJ3156Foj3N2"},"outputs":[],"source":["# The Decoder\n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","    self.dec_layers = [\n","        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","\n","    self.last_attn_scores = None\n","\n","\n","  def call(self, x, context):\n","    # `x` is token-IDs shape (batch, target_seq_len)\n","    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x  = self.dec_layers[i](x, context)\n","\n","    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n","    # The shape of x is (batch_size, target_seq_len, d_model).\n","    return x"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1718702366594,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"dhH1ren8j3N3"},"outputs":[],"source":["# The Transformer\n","class Transformer(tf.keras.Model):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff,\n","               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n","\n","    self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n","\n","    #self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    self.final_layer = tf.keras.layers.Dense(1)\n","\n","\n","  def call(self, inputs):  # Expecting only one input\n","    x = inputs\n","    context = self.encoder(x)\n","    x = self.decoder(x, context)\n","    logits = self.final_layer(x)\n","    try:\n","      del logits._keras_mask\n","    except AttributeError:\n","      pass\n","    return logits"]},{"cell_type":"markdown","metadata":{"id":"nt8LA8qyj3N3"},"source":["**Preparing the data**"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718702172287,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"byevh-BWj3N3"},"outputs":[],"source":["# the categories for prediction\n","final_category = 'blr_mod_lvl'\n","prediction_categories = ['absorption', 'insulation', 't_r_set', 't_out']\n"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":20263,"status":"ok","timestamp":1718703222532,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"Gv-sbnlwj3N3"},"outputs":[],"source":["# Filter data for the 'absorption' category\n","category = 'absorption'\n","filtered_data = data[['house_id', 'time', category]].copy()\n","\n","# We have 1440 minutes per day\n","minutes_per_day = 1440\n","batch_size = 8 # Reduce the batch size, in case it helps !\n","\n","# Function to create input-target pairs for a single house\n","def create_pairs(house_data, category, minutes_per_day=1440):\n","    house_data = house_data.sort_values(by='time')\n","    data = house_data[category].values\n","    days = len(data) // minutes_per_day\n","    data = data[:days * minutes_per_day].reshape((days, minutes_per_day))\n","    input_data = data[:-1]\n","    target_data = data[1:]\n","    return input_data, target_data\n","\n","# Initialize lists to store input and target pairs\n","input_data_list, target_data_list = [], []\n","\n","for house_id in houses:\n","    house_data = filtered_data[filtered_data['house_id'] == 'home14']\n","    input_data, target_data = create_pairs(house_data, category)\n","    input_data_list.append(input_data)\n","    target_data_list.append(target_data)\n","\n","# Combine all houses' data\n","input_data = np.concatenate(input_data_list, axis=0)\n","target_data = np.concatenate(target_data_list, axis=0)\n","\n","# Convert to TensorFlow datasets\n","dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n","dataset = dataset.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","# train - validation - test !!!(80-20, from 80 : 80-20)\n","\n","# Define the Transformer model\n","#model = Transformer(num_layers=4, d_model=128, num_heads=8, dff=512, input_vocab_size=minutes_per_day,\n","#                    target_vocab_size=minutes_per_day, dropout_rate=0.1) # minutes_per_day = 1440\n","model = Transformer(num_layers=4, d_model=64, num_heads=4, dff=256, input_vocab_size=minutes_per_day,\n","                    target_vocab_size=minutes_per_day, dropout_rate=0.1) # minutes_per_day = 1440\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","              loss=tf.keras.losses.MeanSquaredError(),\n","              metrics=[tf.keras.metrics.MeanAbsoluteError()])\n"]},{"cell_type":"code","source":["# Evaluate the model to inspect shapes\n","predictions = model.predict(dataset.take(1)) # Get predictions for one batch\n","print(\"Shape of predictions:\", predictions.shape)\n","print(\"Shape of target batch:\", next(iter(dataset))[1].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q36rI99sojka","executionInfo":{"status":"ok","timestamp":1718703232269,"user_tz":-180,"elapsed":4562,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"83ba5298-5a99-4e34-d472-967b19e41fa6"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 4s 4s/step\n","Shape of predictions: (8, 1440, 1)\n","Shape of target batch: (8, 1440)\n"]}]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4419529,"status":"ok","timestamp":1718707654261,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"XBewxaBNl_Cs","outputId":"f1e4deed-b40f-4962-86ab-7e142f480951"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","599/599 [==============================] - 504s 723ms/step - loss: 35.1366 - mean_absolute_error: 4.8635\n","Epoch 2/10\n","599/599 [==============================] - 432s 721ms/step - loss: 31.1358 - mean_absolute_error: 4.4868\n","Epoch 3/10\n","599/599 [==============================] - 432s 721ms/step - loss: 28.2573 - mean_absolute_error: 4.1841\n","Epoch 4/10\n","599/599 [==============================] - 432s 721ms/step - loss: 24.7433 - mean_absolute_error: 3.8021\n","Epoch 5/10\n","599/599 [==============================] - 433s 722ms/step - loss: 20.8243 - mean_absolute_error: 3.3792\n","Epoch 6/10\n","599/599 [==============================] - 431s 719ms/step - loss: 17.4513 - mean_absolute_error: 2.9996\n","Epoch 7/10\n","599/599 [==============================] - 426s 712ms/step - loss: 15.4389 - mean_absolute_error: 2.7571\n","Epoch 8/10\n","599/599 [==============================] - 426s 711ms/step - loss: 13.2674 - mean_absolute_error: 2.4949\n","Epoch 9/10\n","599/599 [==============================] - 425s 709ms/step - loss: 11.1819 - mean_absolute_error: 2.2384\n","Epoch 10/10\n","599/599 [==============================] - 424s 708ms/step - loss: 11.1515 - mean_absolute_error: 2.2318\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b98e076c4c0>"]},"metadata":{},"execution_count":35}],"source":["# Train the model\n","model.fit(dataset, epochs=10)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}