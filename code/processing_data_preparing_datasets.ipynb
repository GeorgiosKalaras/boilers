{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"wa-Kz67Nj3Ny","executionInfo":{"status":"ok","timestamp":1722156500077,"user_tz":-180,"elapsed":16960,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["import os\n","\n","import csv\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import random\n","\n","import logging\n","import time\n","import tensorflow as tf\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.models import load_model\n","import pickle"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22658,"status":"ok","timestamp":1722156525790,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"onR8HRlXkBzg","outputId":"0152a568-1285-47cd-ed7b-b92b2f171980"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/boilers_drive/normalized_df.csv'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22009,"status":"ok","timestamp":1722156552503,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"EFu6EVSBj3Nz","outputId":"63f919b4-2341-4d0c-b50f-e606e901b5ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imported data:\n","                        time    blr_mod_lvl  absorption  insulation  t_r_set  \\\n","0        2022-10-01 00:00:00   0.000000e+00    0.503910    7.457292     15.0   \n","1        2022-10-01 00:01:00   0.000000e+00    0.503910    7.455208     15.0   \n","2        2022-10-01 00:02:00   0.000000e+00    0.518558    7.487500     15.0   \n","3        2022-10-01 00:03:00   0.000000e+00    0.616207    7.426042     15.0   \n","4        2022-10-01 00:04:00   0.000000e+00    0.699210    7.425000     15.0   \n","...                      ...            ...         ...         ...      ...   \n","7611835  2023-04-30 23:55:00  2.871866e-119    0.000000    1.604167     17.0   \n","7611836  2023-04-30 23:56:00  1.914578e-119    0.000000    1.614583     17.0   \n","7611837  2023-04-30 23:57:00  1.276385e-119    0.000000    1.572917     17.0   \n","7611838  2023-04-30 23:58:00  8.509234e-120    0.000000    1.511458     17.0   \n","7611839  2023-04-30 23:59:00  5.672823e-120    0.000000    1.432203     17.0   \n","\n","             t_out house_id  \n","0        22.342708    home2  \n","1        22.344792    home2  \n","2        22.312500    home2  \n","3        22.373958    home2  \n","4        22.375000    home2  \n","...            ...      ...  \n","7611835  17.395833  home114  \n","7611836  17.385417  home114  \n","7611837  17.427083  home114  \n","7611838  17.488542  home114  \n","7611839  17.567797  home114  \n","\n","[7611840 rows x 7 columns]\n"]}],"source":["# Specify the file path of your CSV file\n","#file_path = 'normalized_df.csv'\n","\n","# the current script directory\n","# the current working directory\n","#current_dir = os.getcwd()\n","\n","# the path to the CSV file in the parent directory\n","#parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n","#file_path = os.path.join(parent_dir, 'normalized_df.csv')\n","\n","# Read the CSV file\n","data = pd.read_csv(file_path)\n","\n","print(\"Imported data:\")\n","print(data)"]},{"cell_type":"code","source":["# Identify NaN values\n","nan_mask = data.isna()\n","print(\"NaN mask:\\n\", nan_mask)\n","\n","# Get the count of NaN values in each column\n","nan_count = data.isna().sum()\n","print(\"\\nNaN count in each column:\\n\", nan_count)\n","\n","# List rows with NaN values\n","rows_with_nan = data[data.isna().any(axis=1)]\n","nan_houses = rows_with_nan['house_id'].unique()\n","print(\"\\nRows with NaN values:\\n\", rows_with_nan)\n","print(\"\\nHouses with NaN values:\\n\", nan_houses)\n","\n","# List columns with NaN values\n","columns_with_nan = data.columns[data.isna().any()].tolist()\n","print(\"\\nColumns with NaN values:\\n\", columns_with_nan)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xsiRhNfui1ye","executionInfo":{"status":"ok","timestamp":1722156560149,"user_tz":-180,"elapsed":5438,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"33befa01-55f4-4c68-d73a-49728256dea9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["NaN mask:\n","           time  blr_mod_lvl  absorption  insulation  t_r_set  t_out  house_id\n","0        False        False       False       False    False  False     False\n","1        False        False       False       False    False  False     False\n","2        False        False       False       False    False  False     False\n","3        False        False       False       False    False  False     False\n","4        False        False       False       False    False  False     False\n","...        ...          ...         ...         ...      ...    ...       ...\n","7611835  False        False       False       False    False  False     False\n","7611836  False        False       False       False    False  False     False\n","7611837  False        False       False       False    False  False     False\n","7611838  False        False       False       False    False  False     False\n","7611839  False        False       False       False    False  False     False\n","\n","[7611840 rows x 7 columns]\n","\n","NaN count in each column:\n"," time           0\n","blr_mod_lvl    0\n","absorption     0\n","insulation     0\n","t_r_set        0\n","t_out          0\n","house_id       0\n","dtype: int64\n","\n","Rows with NaN values:\n"," Empty DataFrame\n","Columns: [time, blr_mod_lvl, absorption, insulation, t_r_set, t_out, house_id]\n","Index: []\n","\n","Houses with NaN values:\n"," []\n","\n","Columns with NaN values:\n"," []\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1722156564205,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-180},"id":"EN_3lAXbj3N0","outputId":"29ad36a3-ec84-4c36-f27b-15d5baa7d375"},"outputs":[{"output_type":"stream","name":"stdout","text":["Different houses in data:\n","['home2' 'home9' 'home13' 'home14' 'home34' 'home46' 'home55' 'home67'\n"," 'home86' 'home93' 'home101' 'home106' 'home110' 'home43' 'home63'\n"," 'home53' 'home79' 'home90' 'home95' 'home5' 'home17' 'home47' 'home51'\n"," 'home65' 'home77' 'home89' 'home111' 'home114']\n","Number of different houses:\n","28\n"]}],"source":["# Different houses in data\n","houses = data['house_id'].unique()\n","print(\"Different houses in data:\")\n","print(houses)\n","print(\"Number of different houses:\")\n","print(len(houses))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"P033elOTj3N0","executionInfo":{"status":"ok","timestamp":1722156587957,"user_tz":-180,"elapsed":20674,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["house_datasets = {}\n","for house in houses:\n","    house_datasets[house] = data[data['house_id'] == house]\n","\n","#print(house_datasets)"]},{"cell_type":"markdown","metadata":{"id":"nt8LA8qyj3N3"},"source":["**Preparing the data**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"byevh-BWj3N3","executionInfo":{"status":"ok","timestamp":1722157731991,"user_tz":-180,"elapsed":2,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["# the categories for prediction\n","final_category = 'blr_mod_lvl'\n","prediction_categories = ['blr_mod_lvl', 'absorption', 'insulation', 't_r_set', 't_out']\n"]},{"cell_type":"code","source":["# alternative for data processing\n","\n","filtered_data = data[['house_id', 'time', 'blr_mod_lvl', 'absorption', 'insulation', 't_r_set', 't_out']].copy()\n","\n","# scaler for each category\n","scaler_blr_mod_lvl = MinMaxScaler()\n","scaler_absorption = MinMaxScaler()\n","scaler_insulation = MinMaxScaler()\n","scaler_t_r_set = MinMaxScaler()\n","scaler_t_out = MinMaxScaler()\n","# dict for scalers\n","scalers = {\n","    \"scaler_blr_mod_lvl\": scaler_blr_mod_lvl,\n","    \"scaler_absorption\": scaler_absorption,\n","    \"scaler_insulation\": scaler_insulation,\n","    \"scaler_t_r_set\": scaler_t_r_set,\n","    \"scaler_t_out\": scaler_t_out\n","}\n","# function to get correct scaler\n","def get_scaler(category_name):\n","    scaler_name = \"scaler_\" + category_name\n","    return scalers.get(scaler_name)\n","\n","# normalizing categories with scalers (all data from all houses in each category)\n","for cat in prediction_categories:\n","    temp = []\n","    temp = filtered_data[cat].values\n","    temp = temp.reshape(-1, 1)  # Reshape to a 2D array with a single column\n","    scaler_temp = get_scaler(cat)\n","    temp_normalized = scaler_temp.fit_transform(temp)\n","    new_cat_name = \"normalized_\" + cat\n","    filtered_data[new_cat_name] = temp_normalized\n","\n","# function for reverse scaling of list (depending on category)\n","def de_scale(cat_list, category):\n","    scaler = get_scaler(category)\n","    return scaler.inverse_transform(cat_list)\n"],"metadata":{"id":"hJr0bDHHgZNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#print(filtered_data)"],"metadata":{"id":"biGUnRrzhqcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating sub-lists, each with the data of one day\n","# the function that gets category data of a house (ordered by date) and separates by day\n","def separate_into_days(data_list, minutes_per_day=1440):\n","    # number of days\n","    num_days = len(data_list) // minutes_per_day\n","    # the data into a list of sub-lists, each containing one day's data\n","    separated_data = [\n","        data_list[i * minutes_per_day:(i + 1) * minutes_per_day]\n","        for i in range(num_days)\n","    ]\n","    return separated_data\n","\n","\n","# function to \"combine\" values of categories and separate into sub-lists based on days\n","def combine_categories(dataset, categories_list):\n","    # Combine specified categories into lists\n","    combined_elements = dataset[categories_list].apply(lambda row: row.tolist(), axis=1)\n","    return combined_elements.tolist()"],"metadata":{"id":"8KH4RCDDgjIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_data_2(house_data, input_categories, output_category, minutes_per_day=1440):\n","    combined_input_data = combine_categories(house_data, input_categories)\n","    separated_input_data = separate_into_days(combined_input_data, minutes_per_day)\n","    output = house_data[output_category].values\n","    separated_output = separate_into_days(output, minutes_per_day)\n","    # [:, :-1] and [:, 1:] for 2-d arrays\n","    # [:-1],  and [1:] for 1-d arrays (or lists)\n","    # all except last day are inputs (for prediction)\n","    input_data = separated_input_data[:-1]\n","    # all except first day are the corresponding outputs (from prediction)\n","    output_data = separated_output[1:]\n","    return input_data, output_data"],"metadata":{"id":"yg0AR2uqapGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gv-sbnlwj3N3"},"outputs":[],"source":["# Filter data for the 'absorption' category\n","#category = 'absorption'\n","filtered_data = filtered_data[['house_id', 'time', 'normalized_blr_mod_lvl', 'normalized_absorption', 'normalized_insulation', 'normalized_t_r_set', 'normalized_t_out']].copy()\n","input_chosen_categories = ['normalized_blr_mod_lvl', 'normalized_absorption']\n","\n","# We have 1440 minutes per day\n","minutes_per_day = 1440\n","\n","# Initialize lists to store input and target pairs\n","input_data_list, target_data_list = [], []\n","\n","for house_id in houses:\n","    house_data = filtered_data[filtered_data['house_id'] == 'home13']\n","    #house_data = filtered_data[filtered_data['house_id'] == house_id]\n","    house_data = house_data.sort_values(by='time')\n","    input_data, target_data = prepare_data_2(house_data, input_chosen_categories, 'normalized_blr_mod_lvl')\n","    input_data_list.append(input_data)\n","    target_data_list.append(target_data)\n","\n","# Combine all houses' data\n","input_data = np.concatenate(input_data_list, axis=0)\n","target_data = np.concatenate(target_data_list, axis=0)\n"]},{"cell_type":"code","source":["# Properly split datasets for train, validation, test\n","# from original data: 80%-20% split, where 20% is for test. from remaining 80% : 80%-20% split, for train and validation\n","\n","batch_size = 8 # Reduce the batch size, in case it helps !\n","\n","# Split the data into train+val and test sets\n","input_train_val, input_test, target_train_val, target_test = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n","\n","# Further split train+val into train and val sets\n","input_train, input_val, target_train, target_val = train_test_split(input_train_val, target_train_val, test_size=0.2, random_state=42)\n","\n","# Convert to TensorFlow datasets\n","train_dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train))\n","train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_dataset = tf.data.Dataset.from_tensor_slices((input_val, target_val))\n","val_dataset = val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((input_test, target_test))\n","test_dataset = test_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n"],"metadata":{"id":"ou2SIOVlg1Bn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input_train.shape)\n","print(target_train.shape)\n","print(input_val.shape)\n","print(target_val.shape)\n","print(input_test.shape)\n","print(target_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHz6ftueyr1S","executionInfo":{"status":"ok","timestamp":1722150337333,"user_tz":-180,"elapsed":322,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"4a6f19fd-4319-4a82-8555-1b55ba526277"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3780, 1440, 2)\n","(3780, 1440)\n","(946, 1440, 2)\n","(946, 1440)\n","(1182, 1440, 2)\n","(1182, 1440)\n"]}]},{"cell_type":"code","source":["# get train-val-test sets and datasets and save for easy retrieval\n"],"metadata":{"id":"2YKFxLBw8q5Z"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}