{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"wa-Kz67Nj3Ny","executionInfo":{"status":"ok","timestamp":1731494931840,"user_tz":-120,"elapsed":15355,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["import os\n","\n","import csv\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import random\n","\n","import logging\n","import time\n","import tensorflow as tf\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.models import load_model\n","import pickle"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38433,"status":"ok","timestamp":1731494970270,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-120},"id":"onR8HRlXkBzg","outputId":"7e91d299-f2d2-4c8e-9d4c-afbe19eefa59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#file_path = '/content/drive/MyDrive/boilers_drive/normalized_df.csv'\n","train_csv_path = '/content/drive/MyDrive/boilers_drive/train_df.csv'\n","val_csv_path = '/content/drive/MyDrive/boilers_drive/val_df.csv'\n","test_csv_path = '/content/drive/MyDrive/boilers_drive/test_df.csv'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EFu6EVSBj3Nz","executionInfo":{"status":"ok","timestamp":1731495005954,"user_tz":-120,"elapsed":35686,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["# Specify the file path of your CSV file\n","\n","# Read the CSV file\n","train_data = pd.read_csv(train_csv_path)\n","val_data = pd.read_csv(val_csv_path)\n","test_data = pd.read_csv(test_csv_path)"]},{"cell_type":"code","source":["#print(train_data)\n","#print(val_data)\n","#print(test_data)"],"metadata":{"id":"75BlObpnUZs9","executionInfo":{"status":"ok","timestamp":1731495005954,"user_tz":-120,"elapsed":2,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# load pre-prepared random order of houses\n","\n","random_order_houses = pd.read_csv('/content/drive/MyDrive/boilers_drive/random_order_houses.csv')"],"metadata":{"id":"FNUeu0fcUcot","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":641,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731495006593,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-120},"id":"EN_3lAXbj3N0","outputId":"67ff575f-21e0-4f64-bf1c-b1f0988de118"},"outputs":[{"output_type":"stream","name":"stdout","text":["Different houses in data:\n","['home9', 'home114', 'home5', 'home89', 'home17', 'home63', 'home2', 'home101', 'home14', 'home95', 'home111', 'home67', 'home77', 'home43', 'home86', 'home90', 'home47', 'home110', 'home93', 'home53', 'home34', 'home51', 'home106', 'home46', 'home79', 'home55', 'home65', 'home13']\n","Number of different houses:\n","28\n"]}],"source":["# Different houses in data\n","houses = random_order_houses['house_id'].unique().tolist()\n","print(\"Different houses in data:\")\n","print(houses)\n","print(\"Number of different houses:\")\n","print(len(houses))"]},{"cell_type":"markdown","metadata":{"id":"fwbpJ2Qmj3N1"},"source":["**Δημιουργία του encoder decoder transformer**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"mQypWHIuj3N2","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":6,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["# positional encoding layer\n","def positional_encoding(length, depth):\n","  depth = depth/2\n","  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","  angle_rates = 1 / (10000**depths)         # (1, depth)\n","  angle_rads = positions * angle_rates      # (pos, depth)\n","\n","  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","# positional embedding layer\n","class PositionalEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, vocab_size, d_model):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.embedding = tf.keras.layers.Dense(d_model)  # Project input to d_model dimension\n","    #self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n","    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n","\n","  def compute_mask(self, *args, **kwargs):\n","    return self.embedding.compute_mask(*args, **kwargs)\n","\n","  def call(self, x):\n","    length = tf.shape(x)[1]\n","    x = self.embedding(x)\n","    # This factor sets the relative scale of the embedding and positonal_encoding.\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x = x + self.pos_encoding[tf.newaxis, :length, :]\n","    return x\n","\n","\n","\n","# Attention layers\n","# These are all identical except for how the attention is configured\n","\n","# base attention layer\n","class BaseAttention(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","    self.add = tf.keras.layers.Add()\n","\n","# cross attention layer\n","# (at the center of the Transformer is the cross-attention layer, it connects the encoder and decoder)\n","class CrossAttention(BaseAttention):\n","  def call(self, x, context):\n","    attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n","    # Cache the attention scores for plotting later.\n","    self.last_attn_scores = attn_scores\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","# global self attention layer\n","# (it is responsible for processing the context sequence, and propagating information along its length)\n","class GlobalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","# causal self attention layer\n","# (it does a similar job as the global self attention layer, for the output sequence)\n","class CausalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","\n","\n","# feed forward network\n","# (the transformer also includes this point-wise feed-forward network in both the encoder and decoder)\n","class FeedForward(tf.keras.layers.Layer):\n","  def __init__(self, d_model, dff, dropout_rate=0.1):\n","    super().__init__()\n","    self.seq = tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),\n","      tf.keras.layers.Dense(d_model),\n","      tf.keras.layers.Dropout(dropout_rate)\n","    ])\n","    self.add = tf.keras.layers.Add()\n","    self.layer_norm = tf.keras.layers.LayerNormalization()\n","\n","  def call(self, x):\n","    x = self.add([x, self.seq(x)])\n","    x = self.layer_norm(x)\n","    return x\n","\n","\n","\n","# encoder layer\n","# (the encoder contains a stack of N encoder layers. Where each EncoderLayer contains\n","#   a GlobalSelfAttention and FeedForward layer)\n","class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x):\n","    x = self.self_attention(x)\n","    x = self.ffn(x)\n","    return x\n","\n","\n","\n","# The Encoder\n","class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","    #self.pos_embedding = PositionalEmbedding(d_model=d_model)\n","    self.enc_layers = [\n","        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","  def call(self, x):\n","    # If x is a tuple, take only the first element\n","    if isinstance(x, tuple):\n","        x = x[0]\n","    # Ensure `x` is shaped as (batch_size, sequence_length)\n","    if x.shape[-1] == 1:\n","        x = tf.squeeze(x, axis=-1)  # Remove the last dimension if its size is 1\n","    # `x` is token-IDs shape: (batch, seq_len)\n","    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    # Add dropout.\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x)\n","    return x  # Shape `(batch_size, seq_len, d_model)`.\n","\n","\n","\n","# decoder layer\n","# (the decoder's stack is slightly more complex, with each DecoderLayer containing\n","#   a CausalSelfAttention, a CrossAttention, and a FeedForward layer)\n","class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","    self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x, context):\n","    x = self.causal_self_attention(x=x)\n","    x = self.cross_attention(x=x, context=context)\n","    # Cache the last attention scores for plotting later\n","    self.last_attn_scores = self.cross_attention.last_attn_scores\n","    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    return x\n","\n","\n","\n","# The Decoder\n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","    #self.pos_embedding = PositionalEmbedding(d_model=d_model)\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","    self.dec_layers = [\n","        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","    self.last_attn_scores = None\n","\n","  def call(self, x, context):\n","    # If x is a tuple, take only the first element\n","    if isinstance(x, tuple):\n","        x = x[0]\n","    # Ensure `x` is shaped as (batch_size, sequence_length)\n","    if x.shape[-1] == 1:\n","        x = tf.squeeze(x, axis=-1)  # Remove the last dimension if its size is 1\n","    # `x` is token-IDs shape (batch, target_seq_len)\n","    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x  = self.dec_layers[i](x, context)\n","    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n","    # The shape of x is (batch_size, target_seq_len, d_model).\n","    return x\n","\n","\n","\n","# The Transformer\n","@tf.keras.utils.register_keras_serializable()\n","class Transformer(tf.keras.Model):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff,\n","               input_vocab_size, target_vocab_size, dropout_rate=0.1, **kwargs):\n","    super().__init__(**kwargs)\n","\n","    self.input_proj = tf.keras.layers.Dense(d_model)  # Project input to the model dimension\n","    self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n","    self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n","    #self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    self.final_layer = tf.keras.layers.Dense(1)\n","\n","  #def call(self, inputs, decoder_input=None):  # Expect separate decoder input\n","  #  x = inputs\n","  #  context = self.encoder(x)\n","  #  # If no decoder input is provided, assume we're in inference mode\n","  #  if decoder_input is None:\n","  #      # Initialize decoder input with zeros (batch_size, seq_len, d_model)\n","  #      decoder_input = tf.zeros_like(inputs)\n","  #  x = self.decoder(decoder_input, context)\n","  #  logits = self.final_layer(x)\n","  #  try:\n","  #      del logits._keras_mask\n","  #  except AttributeError:\n","  #      pass\n","  #  return logits\n","\n","  def call(self, inputs, decoder_input=None, training=False):\n","    # Unpack the tuple correctly if necessary\n","    if isinstance(inputs, tuple):\n","        encoder_input, decoder_input = inputs\n","\n","    # Proceed with the encoder and decoder as expected\n","    # Encode the input sequence\n","    context = self.encoder(inputs)\n","\n","    # Ensure that `decoder_input` is properly set even if None\n","    if decoder_input is None:\n","        batch_size = tf.shape(inputs)[0]\n","        decoder_input = tf.zeros((batch_size, 1, self.d_model))  # Initialized with zeros\n","\n","    if training:\n","        # In training mode, we use the provided decoder input\n","        decoder_output = self.decoder(decoder_input, context)\n","        logits = self.final_layer(decoder_output)\n","    else:\n","        # For auto-regressive generation over 1440 time steps\n","        decoder_output = []\n","\n","        for t in range(1440):  # Loop over each time step for prediction\n","            # Run through the decoder to get predictions for this step\n","            pred = self.decoder(decoder_input, context)\n","            output = self.final_layer(pred)  # Shape: (batch_size, 1, output_dim)\n","            decoder_output.append(output)\n","            # Update `decoder_input` for the next time step\n","            decoder_input = output\n","\n","        # Concatenate outputs along the time dimension\n","        decoder_output = tf.concat(decoder_output, axis=1)  # Shape: (batch_size, 1440, output_dim)\n","        logits = decoder_output\n","\n","    # Clear any masking that might interfere with training\n","    try:\n","        del logits._keras_mask\n","    except AttributeError:\n","        pass\n","\n","    return logits\n"]},{"cell_type":"markdown","metadata":{"id":"nt8LA8qyj3N3"},"source":["**Preparing the data**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"byevh-BWj3N3","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":6,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["# the categories for prediction\n","final_category = 'blr_mod_lvl'\n","prediction_categories = ['blr_mod_lvl', 'absorption', 'insulation', 't_r_set', 't_out']\n","normalized_categories = ['normalized_blr_mod_lvl', 'normalized_absorption', 'normalized_insulation', 'normalized_t_r_set', 'normalized_t_out']"]},{"cell_type":"code","source":["# creating sub-lists, each with the data of one day\n","# the function that gets category data of a house (ordered by date) and separates by day\n","def separate_into_days(data_list, minutes_per_day=1440):\n","    # number of days\n","    num_days = len(data_list) // minutes_per_day\n","    # the data into a list of sub-lists, each containing one day's data\n","    separated_data = [\n","        data_list[i * minutes_per_day:(i + 1) * minutes_per_day]\n","        for i in range(num_days)\n","    ]\n","    return separated_data\n","\n","\n","# function to \"combine\" values of categories and separate into sub-lists based on days\n","def combine_categories(dataset, categories_list):\n","    # Combine specified categories into lists\n","    combined_elements = dataset[categories_list].apply(lambda row: row.tolist(), axis=1)\n","    return combined_elements.tolist()"],"metadata":{"id":"8KH4RCDDgjIF","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":5,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def prepare_data_2(house_data, input_categories, output_category, minutes_per_day=1440):\n","    combined_input_data = combine_categories(house_data, input_categories)\n","    separated_input_data = separate_into_days(combined_input_data, minutes_per_day)\n","    #output = house_data[output_category].values\n","    output = combine_categories(house_data, [output_category])\n","    separated_output = separate_into_days(output, minutes_per_day)\n","\n","    input_data = separated_input_data[:-1]  # All except the last day for encoder input\n","    target_data = separated_output[1:]      # All except the first day for target output\n","    # For decoder input, shift target by one step\n","    decoder_input_data = separated_output[:-1]  # The previous day's output as decoder input\n","\n","    return input_data, decoder_input_data, target_data"],"metadata":{"id":"yg0AR2uqapGc","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":5,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Gv-sbnlwj3N3","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":5,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["# function for getting input and target data\n","def input_target_split (data, input_categories, selected_houses):\n","    filtered_data = data[['house_id', 'time', 'normalized_blr_mod_lvl', 'normalized_absorption', 'normalized_insulation', 'normalized_t_r_set', 'normalized_t_out']].copy()\n","    input_chosen_categories = []\n","    for cat in input_categories:\n","        input_chosen_categories.append('normalized_'+cat)\n","\n","    # We have 1440 minutes per day\n","    minutes_per_day = 1440\n","\n","    # Initialize lists to store input and target pairs\n","    input_data_list, target_data_list, decoder_input_list = [], [], []\n","\n","    for house_id in selected_houses:\n","        house_data = filtered_data[filtered_data['house_id'] == house_id]\n","        house_data = house_data.sort_values(by='time')\n","        input_data, decoder_input_data, target_data = prepare_data_2(house_data, input_chosen_categories, 'normalized_blr_mod_lvl')\n","        input_data_list.append(input_data)\n","        decoder_input_list.append(decoder_input_data)\n","        target_data_list.append(target_data)\n","\n","    # Combine all houses' data\n","    input_data = np.concatenate(input_data_list, axis=0)\n","    decoder_input_data = np.concatenate(decoder_input_list, axis=0)\n","    target_data = np.concatenate(target_data_list, axis=0)\n","\n","    return input_data, decoder_input_data, target_data\n"]},{"cell_type":"code","source":["def get_split_data(train_data, val_data, input_categories, selected_houses):\n","    input_train, decoder_input_train, target_train = input_target_split(train_data, input_categories, selected_houses)\n","    input_val, decoder_input_val, target_val = input_target_split(val_data, input_categories, selected_houses)\n","    #input_test, decoder_input_test, target_test = input_target_split(test_data, input_categories, selected_houses)\n","    return input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val"],"metadata":{"id":"4ujpGxLKU9R9","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":5,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["batch_size=8"],"metadata":{"id":"m6SLLa-zhX6s","executionInfo":{"status":"ok","timestamp":1731495006593,"user_tz":-120,"elapsed":5,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def get_datasets(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val, batch_size=batch_size):\n","    # Reduce the batch size, in case it helps !\n","    # Convert to TensorFlow datasets\n","    train_dataset = tf.data.Dataset.from_tensor_slices(((input_train, decoder_input_train), target_train))\n","    #train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    train_dataset = train_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","    val_dataset = tf.data.Dataset.from_tensor_slices(((input_val, decoder_input_val), target_val))\n","    val_dataset = val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","    # !!! we do this for test_data separatelly later\n","\n","    # The validation_split argument is designed to work with NumPy arrays or TensorFlow tensors,\n","    # where it can easily split the data based on a fraction.\n","    # It doesn't directly work with TensorFlow datasets because they handle data differently.\n","    return train_dataset, val_dataset"],"metadata":{"id":"ou2SIOVlg1Bn","executionInfo":{"status":"ok","timestamp":1731495006594,"user_tz":-120,"elapsed":5,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def print_shapes(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val):\n","    print(\"For train data: input, decoder_input, target  shapes are:\")\n","    print(input_train.shape)\n","    print(decoder_input_train.shape)\n","    print(target_train.shape)\n","    print(\"For val data: input, decoder_input, target  shapes are:\")\n","    print(input_val.shape)\n","    print(decoder_input_val.shape)\n","    print(target_val.shape)\n","    num_of_categories = input_train.shape[-1]\n","    print(\"Number of categories for prediction: \"+str(num_of_categories))\n","    return num_of_categories"],"metadata":{"id":"eHz6ftueyr1S","executionInfo":{"status":"ok","timestamp":1731495007206,"user_tz":-120,"elapsed":616,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# to calculate the R2 during training and predicting\n","def r2_score(y_true, y_pred):\n","    # Flatten the arrays to compute R2 on all predicted values at once\n","    y_true_flat = tf.reshape(y_true, shape=(-1,))\n","    y_pred_flat = tf.reshape(y_pred, shape=(-1,))\n","\n","    # Ensure both tensors are of the same type (float32 in this case)\n","    y_true_flat = tf.cast(y_true_flat, tf.float32)\n","    y_pred_flat = tf.cast(y_pred_flat, tf.float32)\n","\n","    # Calculate R2\n","    ss_res = tf.reduce_sum(tf.square(y_true_flat - y_pred_flat))\n","    ss_tot = tf.reduce_sum(tf.square(y_true_flat - tf.reduce_mean(y_true_flat)))\n","\n","    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon()) # add small epsilon constant to avoid diivision by 0"],"metadata":{"id":"P7gaVhqy7uyC","executionInfo":{"status":"ok","timestamp":1731495007206,"user_tz":-120,"elapsed":4,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"FeNznSxDx0kY","executionInfo":{"status":"ok","timestamp":1731495007206,"user_tz":-120,"elapsed":3,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"outputs":[],"source":["def make_transformer_model():\n","    minutes_per_day = 1440\n","    # Define the Transformer model\n","    #model = Transformer(num_layers=4, d_model=128, num_heads=8, dff=512, input_vocab_size=minutes_per_day,\n","    #                    target_vocab_size=minutes_per_day, dropout_rate=0.1) # minutes_per_day = 1440\n","    model = Transformer(num_layers=4, d_model=64, num_heads=4, dff=256, input_vocab_size=minutes_per_day,\n","                        target_vocab_size=minutes_per_day, dropout_rate=0.1) # minutes_per_day = 1440\n","    # Compile the model\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","                  loss=tf.keras.losses.MeanSquaredError(),\n","                  metrics=[tf.keras.metrics.MeanAbsoluteError(), r2_score])\n","    return model"]},{"cell_type":"code","source":["def train_transformer_model(model, train_dataset, val_dataset):\n","    # Define the EarlyStopping callback\n","    early_stopping = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',  # Metric to monitor\n","        patience=5,          # Number of epochs with no improvement after which training will be stopped\n","        verbose=1,           # Verbosity mode\n","        restore_best_weights=True  # Whether to restore model weights from the epoch with the best value of the monitored quantity\n","    )\n","    # Train the model with the EarlyStopping callback\n","    history = model.fit(\n","        train_dataset,       # Training dataset\n","        epochs=20,          # Number of epochs\n","        validation_data=val_dataset,  # Validation dataset\n","        callbacks=[early_stopping]  # List of callbacks to apply during training\n","    )\n","    # change epochs, patience , etc !!! (e.g. epochs=10, patience=3  or  epochs=20, patience=5)\n","    return model, history"],"metadata":{"id":"lzVc2U2WfVQi","executionInfo":{"status":"ok","timestamp":1731495007206,"user_tz":-120,"elapsed":3,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def training_plots(history):\n","    # Plotting the training and validation loss and MAE\n","    plt.figure(figsize=(14, 5))\n","\n","    # Plot training & validation loss values\n","    plt.subplot(2, 2, 1)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Model Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss (MSE)')\n","    plt.legend(loc='upper right')\n","\n","    # Plot training & validation MAE values\n","    plt.subplot(2, 2, 2)\n","    plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n","    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n","    plt.title('Model Mean Absolute Error')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('MAE')\n","    plt.legend(loc='upper right')\n","\n","    # Plot training & validation R2 values\n","    plt.subplot(2, 2, 3)\n","    plt.plot(history.history['r2_score'], label='Training R2')\n","    plt.plot(history.history['val_r2_score'], label='Validation R2')\n","    plt.title('Model R2 Score')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('R2 Score')\n","    plt.legend(loc='upper right')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"EIxdGKklzwG7","executionInfo":{"status":"ok","timestamp":1731495007206,"user_tz":-120,"elapsed":2,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def save_transformer_model(model, history, input_categories, num_of_houses, save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/models/'):\n","    # save the model\n","    # !!! I need to save the scalers and the input_test, target_test, test_dataset as well !!!\n","    # !!! Or I need to use the same for all models (save once and use for all) !!!\n","    categories_used = '_'.join(input_categories)\n","    houses_used = str(num_of_houses)\n","\n","    model_name = 'transformer_'+categories_used+\"_houses_\"+houses_used\n","\n","    # save trained model\n","    model.save(save_path + model_name + '.keras')\n","\n","    # save history of training\n","    with open(save_path + model_name + '_history.pkl', 'wb') as f:\n","      pickle.dump(history.history, f)"],"metadata":{"id":"sBGuQ4wCWr0O","executionInfo":{"status":"ok","timestamp":1731495007206,"user_tz":-120,"elapsed":2,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Load the scalers_dict\n","with open('/content/drive/MyDrive/boilers_drive/scalers.pkl', 'rb') as file:\n","    scalers_dict = pickle.load(file)"],"metadata":{"id":"j2fl98e_8DJS","executionInfo":{"status":"ok","timestamp":1731495008236,"user_tz":-120,"elapsed":1033,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def get_test_values(test_values, house, category='blr_mod_lvl'):\n","    flattened_test_values = test_values.reshape(-1)\n","    if category=='blr_mod_lvl':\n","      de_scaled_test_values = (flattened_test_values.reshape(-1, 1)) * 100\n","      # de-scaling is multiplying by 100 for blr_mod_lvl values\n","      final_test_values = de_scaled_test_values.reshape(-1)\n","      #final_test_values = list(map(int, de_scaled_test_values))\n","    else:\n","      scaler = scalers_dict[house][category]\n","      test_values_reshaped = test_values.reshape(-1, 1)\n","      de_scaled_values = scaler.inverse_transform(test_values_reshaped)\n","      final_test_values = de_scaled_values.flatten()  # flatten de-scaled values\n","    return final_test_values\n","\n","# get predictions in list form per house\n","#def get_predictions(model, test_dataset, house, category='blr_mod_lvl'):\n","#    test_predictions = model.predict(test_dataset)\n","#    #print(\"predictions shape: \"+str(test_predictions.shape))\n","#    prediction = get_test_values(test_predictions, house, category)\n","#    #print(\"after processing, prediction shape: \"+str(prediction.shape))\n","#    return prediction\n","\n","def get_predictions(model, input_test, decoder_input_test, house, category='blr_mod_lvl'):\n","    # Predict using encoder and decoder inputs\n","    test_predictions = model.predict((input_test, decoder_input_test), training=False)\n","    # De-scale and flatten the predictions\n","    prediction = get_test_values(test_predictions, house, category)\n","    return prediction"],"metadata":{"id":"kuYVz2ANcY9a","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":8,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# function that gives us the error metrics for given pair: target, prediction\n","def get_error_metrics(target, prediction):\n","    # error values\n","    error = [t - p for t, p in zip(target, prediction)]\n","    AE = [abs(e) for e in error] # Absolute Error\n","    SE = [e ** 2 for e in error] # Squared Error\n","\n","    # error metrics\n","    # Mean Absolute Error, gives magnitude of errors without caring for direction\n","    MAE = np.mean(AE)\n","    # Mean Squared Error, gives higher weight for larger errors\n","    MSE = np.mean(SE)\n","    # Root Mean Squared Error,  it is in the same units as the target variable\n","    RMSE = np.sqrt(MSE)\n","    # R-Squared, statistical measure that represents the proportion of the variance for the target variable that's explained by the model\n","    # provides an indication of the goodness of fit\n","    mean_target = np.mean(target)\n","    diff = [t - mean_target for t in target]\n","    denominator = [d ** 2 for d in diff]\n","    R2 = 1 - (sum(SE) / sum(denominator))\n","\n","    return [MAE, MSE, RMSE, R2]"],"metadata":{"id":"OgLdST1bdJLj","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":7,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def save_predictions_to_csv(target_values, prediction_values, model_name, house, save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'):\n","    folder_name = model_name + '_predictions'\n","    folder_path = os.path.join(save_path, folder_name)\n","\n","    # Check if the folder exists, and create it if it doesn't\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","        #print(f\"Folder created: {folder_path}\")\n","    #else:\n","        #print(f\"Folder already exists: {folder_path}\")\n","\n","    # the dataframe with target and prediction values\n","    df = pd.DataFrame({\n","        'Target': target_values,\n","        'Prediction': prediction_values\n","    })\n","\n","    file_name = house+'predictions.csv'\n","    file_path = os.path.join(folder_path, file_name)\n","\n","    # Save as a CSV file\n","    df.to_csv(file_path, index=False)"],"metadata":{"id":"p4PfH3jP8RM1","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":7,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# get test_dataset (only)\n","def get_test_dataset(input_test, decoder_input_test, target_test, batch_size=batch_size):\n","    # Reduce the batch size, in case it helps !\n","    test_dataset = tf.data.Dataset.from_tensor_slices(((input_test, decoder_input_test), target_test))\n","    test_dataset = test_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    return test_dataset\n","\n","# get test input and target for every house (separately) for given input_categories\n","#def get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl'):\n","#    test_values_per_house = {}\n","#    test_dataset_per_house = {}\n","#    for house in houses:\n","#        input_test, decoder_input_test, target_test = input_target_split(test_data[test_data[\"house_id\"]==house], input_categories, [house])\n","#        #print(\"for house with id: \"+house)\n","#        #print(\"input_test shape: \"+str(input_test.shape))\n","#        #print(\"target_test shape: \"+str(target_test.shape))\n","#        test_values = get_test_values(target_test, house, category)\n","#        test_values_per_house[house] = test_values\n","#        test_dataset_per_house[house] = get_test_dataset(input_test, decoder_input_test, target_test, batch_size=batch_size)\n","#    return test_values_per_house, test_dataset_per_house\n","\n","\n","def get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl'):\n","    test_values_per_house = {}\n","    test_input_per_house = {}\n","    test_decoder_input_per_house = {}\n","    for house in houses:\n","        # Retrieve encoder input, decoder input, and target\n","        input_test, decoder_input_test, target_test = input_target_split(test_data[test_data[\"house_id\"] == house], input_categories, [house])\n","\n","        test_values = get_test_values(target_test, house, category)\n","        test_values_per_house[house] = test_values\n","        test_input_per_house[house] = input_test\n","        test_decoder_input_per_house[house] = decoder_input_test\n","    return test_values_per_house, test_input_per_house, test_decoder_input_per_house\n","\n","\n","# function which calculates the average metrics for some houses\n","def calculate_average_metrics(metrics_per_house, selected_houses):\n","    MAE_sum, MSE_sum, RMSE_sum, R2_sum = 0, 0, 0, 0\n","    n = len(selected_houses)\n","    for house in selected_houses:\n","        [MAE, MSE, RMSE, R2] = metrics_per_house[house]\n","        MAE_sum += MAE\n","        MSE_sum += MSE\n","        RMSE_sum += RMSE\n","        R2_sum += R2\n","\n","    MAE_avg = MAE_sum / n\n","    MSE_avg = MSE_sum / n\n","    RMSE_avg = RMSE_sum / n\n","    R2_avg = R2_sum / n\n","\n","    return [MAE_avg, MSE_avg, RMSE_avg, R2_avg]\n","\n","# get prediction metrics per house for a model\n","# (assume correct input_categories for model and datasets per house)\n","# it also gives the avg metrics for relevant houses (the houses with which the model was trained) and all houses\n","#def get_prediction_metrics_per_house(model, model_name, num_of_houses, test_values_per_house, test_dataset_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'):\n","#    prediction_per_house = {}\n","#    error_metrics_per_house = {}\n","#    for house in houses:\n","#        prediction_per_house[house] = get_predictions(model, test_dataset_per_house[house], house, category)\n","#        save_predictions_to_csv(test_values_per_house[house], prediction_per_house[house], model_name, house, save_path)\n","#        [MAE, MSE, RMSE, R2] = get_error_metrics(test_values_per_house[house], prediction_per_house[house])\n","#        error_metrics_per_house[house] = [MAE, MSE, RMSE, R2]\n","#    for special in [\"relevant_houses\", \"all_houses\"]:\n","#        if (special==\"relevant_houses\"):\n","#            selected_houses = houses[:num_of_houses]\n","#        else: # in this case we use all the houses\n","#            selected_houses = houses\n","#        error_metrics_per_house[special] = calculate_average_metrics(error_metrics_per_house, selected_houses)\n","#    return error_metrics_per_house\n","\n","def get_prediction_metrics_per_house(model, model_name, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'):\n","    prediction_per_house = {}\n","    error_metrics_per_house = {}\n","    for house in houses:\n","        prediction_per_house[house] = get_predictions(model, test_input_per_house[house], test_decoder_input_per_house[house], house, category)\n","        save_predictions_to_csv(test_values_per_house[house], prediction_per_house[house], model_name, house, save_path)\n","        [MAE, MSE, RMSE, R2] = get_error_metrics(test_values_per_house[house], prediction_per_house[house])\n","        error_metrics_per_house[house] = [MAE, MSE, RMSE, R2]\n","    for special in [\"relevant_houses\", \"all_houses\"]:\n","        if (special==\"relevant_houses\"):\n","            selected_houses = houses[:num_of_houses]\n","        else:\n","            selected_houses = houses\n","        error_metrics_per_house[special] = calculate_average_metrics(error_metrics_per_house, selected_houses)\n","    return error_metrics_per_house\n","\n","\n","# get prediction metrics per house for a model in dataframe form\n","def get_metrics_dataframe(model_name, error_metrics_per_house, categories_str, num_of_houses, num_of_categories):\n","    # along with the individual houses we also have average metrics for the model\n","    # we add first the \"relevant\" and \"all\" before the individual houses in the list\n","    used_houses = [\"relevant_houses\", \"all_houses\"]\n","    for house in houses:\n","        used_houses.append(house)\n","\n","    data = {}\n","    data['categories'] = [categories_str]\n","    data['num_of_categories'] = [num_of_categories]\n","    data['num_of_houses'] = [num_of_houses]\n","    # Constructing the column names and filling them with metrics\n","    for house in used_houses:\n","        metrics = error_metrics_per_house[house]\n","        data['MAE_'+house] = [metrics[0]]\n","        data['MSE_'+house] = [metrics[1]]\n","        data['RMSE_'+house] = [metrics[2]]\n","        data['R2_'+house] = [metrics[3]]\n","\n","    # Create the dataframe\n","    new_house_df = pd.DataFrame(data, index=[model_name])\n","    return new_house_df"],"metadata":{"id":"srfW81E2mkv-","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":7,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# loading the transformer error metrics (until now) from csv and adding the new house (as a new row)\n","def update_transformer_error_metrics(new_house_df, model_name, path_to_transformer_error_metrics='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'):\n","    # load the existing dataframe from csv file\n","    try:\n","        transformer_error_metrics = pd.read_csv(path_to_transformer_error_metrics, index_col=0)\n","    except FileNotFoundError:\n","        # if the file does not exist, creates a new dataframe\n","        print(\"File not found. Creating a new one.\")\n","        transformer_error_metrics = new_house_df # there is no file, so the 'new' is made the 'transformer_error_metrics' file by default\n","        # save the updated dataframe back to csv file\n","        transformer_error_metrics.to_csv(path_to_transformer_error_metrics)\n","        return transformer_error_metrics\n","\n","    # align columns between existing dataframe and the new dataframe and then concatenate\n","    new_house_df = new_house_df.reindex(columns=transformer_error_metrics.columns, fill_value=pd.NA) # missing columns will be filled with value: 'NaN'\n","    # check if the model is already in the existing dataframe\n","    if model_name in transformer_error_metrics.index:\n","        # update the existing row with new values\n","        transformer_error_metrics.loc[model_name] = new_house_df.iloc[0]\n","    else:\n","        # append the new row if the model does not exist\n","        transformer_error_metrics = pd.concat([transformer_error_metrics, new_house_df], axis=0)\n","\n","    # save the updated dataframe back to csv file\n","    transformer_error_metrics = transformer_error_metrics.sort_values(by=['num_of_categories', 'categories', 'num_of_houses'])\n","    transformer_error_metrics.to_csv(path_to_transformer_error_metrics)\n","\n","    return transformer_error_metrics"],"metadata":{"id":"lF50zqwjn0ym","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":7,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["#path_to_transformer_error_metrics = '/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'\n","#transformer_error_metrics = pd.read_csv(path_to_transformer_error_metrics, index_col=0)\n","#transformer_error_metrics = transformer_error_metrics.sort_values(by=['num_of_categories', 'categories', 'num_of_houses'])\n","#print(transformer_error_metrics.to_string(index=True))"],"metadata":{"id":"lE0NYSzhfTsZ","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":7,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# do it all completely\n","def transformer_model_complete(train_data, val_data, input_categories, num_of_houses, batch_size):\n","    print(\"splitting data\")\n","    selected_houses = houses[:num_of_houses]\n","    input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val = get_split_data(train_data, val_data, input_categories, selected_houses)\n","    print(\"getting datasets\")\n","    train_dataset, val_dataset = get_datasets(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val, batch_size=batch_size)\n","    print(\"printing shapes and getting num_of_categories\")\n","    num_of_categories = print_shapes(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val)\n","    print(\"making transformer model\")\n","    model = make_transformer_model()\n","    print(\"training transformer model\")\n","    model, history = train_transformer_model(model, train_dataset, val_dataset)\n","    print(\"the training plots\")\n","    training_plots(history)\n","    #print(\"saving the model and the training history\")\n","    #save_transformer_model(model, history, input_categories, num_of_houses)\n","    return model, history, input_categories, num_of_houses\n","\n","def doing_testing(model, input_categories, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/', path_to_transformer_error_metrics='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'):\n","    num_of_categories = len(input_categories)\n","    categories_used = '_'.join(input_categories)\n","    houses_used = str(num_of_houses)\n","    model_name = 'transformer_'+categories_used+\"_houses_\"+houses_used\n","    print(\"getting error metrics\")\n","    error_metrics_per_house = get_prediction_metrics_per_house(model, model_name, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category, save_path)\n","    new_house_df = get_metrics_dataframe(model_name, error_metrics_per_house, categories_used, num_of_houses, num_of_categories)\n","    print(\"updating the transformer error metrics\")\n","    transformer_error_metrics = update_transformer_error_metrics(new_house_df, model_name, path_to_transformer_error_metrics) # path for load/store is already given\n","    print(\"the new transformer error metrics file:\")\n","    print(transformer_error_metrics.to_string(index=True))\n","    print(\"--------------------\")\n","    print(\"finished\")\n"],"metadata":{"id":"uxkp7TNYXHTG","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":6,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["print(prediction_categories)\n","\n","input_categories = [prediction_categories[0]]\n","\n","num_of_houses = 1\n","#num_of_houses = 7\n","#num_of_houses = len(houses)\n","\n","possible_num_of_houses = [1, 4, len(houses)] # 1 house, 4 houses (which is about 1/8 of all 28 houses), all houses (28 houses)\n","\n","print(input_categories)\n","print(num_of_houses)\n","print(possible_num_of_houses)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJHKUtGNXpGu","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":6,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"edb1eaad-bdc3-4d81-acc8-bcadc890c244"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["['blr_mod_lvl', 'absorption', 'insulation', 't_r_set', 't_out']\n","['blr_mod_lvl']\n","1\n","[1, 4, 28]\n"]}]},{"cell_type":"code","source":["print(prediction_categories[:1])\n","print(possible_num_of_houses[:1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9U7smFY7rPqR","executionInfo":{"status":"ok","timestamp":1731495008237,"user_tz":-120,"elapsed":5,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"d5ae1a64-c259-4dfd-8dfd-fac24e33a168"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["['blr_mod_lvl']\n","[1]\n"]}]},{"cell_type":"code","source":["# !!! all models have been trained and saved (problems with loading saved models) !!!\n","# !!! geting error_metrics !!!\n","batch_size=8\n","#batch_size=16 # it is a bit slow, for greater batch_sizes there is error\n","\n","# ! Metrics are shown for the last batch\n","\n","for input_category in prediction_categories[:1]:\n","    input_categories = [input_category]\n","    #test_values_per_house, test_dataset_per_house = get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl')\n","    test_values_per_house, test_input_per_house, test_decoder_input_per_house = get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl')\n","    for num_of_houses in possible_num_of_houses[:1]:\n","        print(\"---NEW Transformer MODEL---\")\n","        print(\"input_categories: \"+str(input_categories))\n","        print(\"num_of_houses: \"+str(num_of_houses))\n","        print(\"--------------------\")\n","        model, history, input_categories, num_of_houses = transformer_model_complete(train_data, val_data, input_categories, num_of_houses, batch_size=batch_size)\n","        doing_testing(model, input_categories, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/', path_to_transformer_error_metrics='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv')\n","        print(\"--------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mJ5-8TyaX3w-","outputId":"3d431472-e5a5-45fe-d09e-c8eb2e2cbbe9","executionInfo":{"status":"error","timestamp":1731495074048,"user_tz":-120,"elapsed":65814,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["---NEW Transformer MODEL---\n","input_categories: ['blr_mod_lvl']\n","num_of_houses: 1\n","--------------------\n","splitting data\n","getting datasets\n","printing shapes and getting num_of_categories\n","For train data: input, decoder_input, target  shapes are:\n","(134, 1440, 1)\n","(134, 1440, 1)\n","(134, 1440, 1)\n","For val data: input, decoder_input, target  shapes are:\n","(33, 1440, 1)\n","(33, 1440, 1)\n","(33, 1440, 1)\n","Number of categories for prediction: 1\n","making transformer model\n","training transformer model\n","Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'transformer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node gradient_tape/transformer_1/decoder_1/positional_embedding_1_1/add/BroadcastGradientArgs defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [8,64] vs. [1,1440,64]\n\nStack trace for op definition: \nFile \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\nFile \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\nFile \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\nFile \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\nFile \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\nFile \"<ipython-input-31-6e2bbd9fd975>\", line 17, in <cell line: 8>\nFile \"<ipython-input-28-1e7803f8e847>\", line 13, in transformer_model_complete\nFile \"<ipython-input-18-c59fd515f6ae>\", line 10, in train_transformer_model\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\n\n\t [[{{node gradient_tape/transformer_1/decoder_1/positional_embedding_1_1/add/BroadcastGradientArgs}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_41446[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_42561]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-6e2bbd9fd975>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_of_houses: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_houses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_houses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_model_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_houses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdoing_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_houses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_values_per_house\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_per_house\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_decoder_input_per_house\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blr_mod_lvl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_transformer_error_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-1e7803f8e847>\u001b[0m in \u001b[0;36mtransformer_model_complete\u001b[0;34m(train_data, val_data, input_categories, num_of_houses, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training transformer model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the training plots\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtraining_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-c59fd515f6ae>\u001b[0m in \u001b[0;36mtrain_transformer_model\u001b[0;34m(model, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Train the model with the EarlyStopping callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# Training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/transformer_1/decoder_1/positional_embedding_1_1/add/BroadcastGradientArgs defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [8,64] vs. [1,1440,64]\n\nStack trace for op definition: \nFile \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\nFile \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\nFile \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\nFile \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\nFile \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\nFile \"<ipython-input-31-6e2bbd9fd975>\", line 17, in <cell line: 8>\nFile \"<ipython-input-28-1e7803f8e847>\", line 13, in transformer_model_complete\nFile \"<ipython-input-18-c59fd515f6ae>\", line 10, in train_transformer_model\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\n\n\t [[{{node gradient_tape/transformer_1/decoder_1/positional_embedding_1_1/add/BroadcastGradientArgs}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_41446[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_42561]"]}]},{"cell_type":"code","source":["path_to_transformer_error_metrics = '/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'\n","transformer_error_metrics = pd.read_csv(path_to_transformer_error_metrics, index_col=0)\n","transformer_error_metrics = transformer_error_metrics.sort_values(by=['num_of_houses', 'categories', 'num_of_categories'])\n","print(transformer_error_metrics.to_string(index=True))"],"metadata":{"id":"Yl4wENP7Deld","executionInfo":{"status":"aborted","timestamp":1731491012052,"user_tz":-120,"elapsed":4,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**------------------------------------------------------------------**\n","\n","---\n","**-------------------Μερικές Δοκιμές---------------------------**\n","\n","---\n","**-------------------------------------------------------------------**"],"metadata":{"id":"9Dg5VHpypiBE"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}