{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wa-Kz67Nj3Ny"},"outputs":[],"source":["import os\n","\n","import csv\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import random\n","\n","import logging\n","import time\n","import tensorflow as tf\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.models import load_model\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19972,"status":"ok","timestamp":1730887522439,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-120},"id":"onR8HRlXkBzg","outputId":"1141d8d2-b286-4761-9b5e-d95497400144"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#file_path = '/content/drive/MyDrive/boilers_drive/normalized_df.csv'\n","train_csv_path = '/content/drive/MyDrive/boilers_drive/train_df.csv'\n","val_csv_path = '/content/drive/MyDrive/boilers_drive/val_df.csv'\n","test_csv_path = '/content/drive/MyDrive/boilers_drive/test_df.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFu6EVSBj3Nz"},"outputs":[],"source":["# Specify the file path of your CSV file\n","\n","# Read the CSV file\n","train_data = pd.read_csv(train_csv_path)\n","val_data = pd.read_csv(val_csv_path)\n","test_data = pd.read_csv(test_csv_path)"]},{"cell_type":"code","source":["#print(train_data)\n","#print(val_data)\n","#print(test_data)"],"metadata":{"id":"75BlObpnUZs9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load pre-prepared random order of houses\n","\n","random_order_houses = pd.read_csv('/content/drive/MyDrive/boilers_drive/random_order_houses.csv')"],"metadata":{"id":"FNUeu0fcUcot"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1730887562561,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"},"user_tz":-120},"id":"EN_3lAXbj3N0","outputId":"2c9887e5-b3d4-4e25-f07e-b9af886183a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Different houses in data:\n","['home9', 'home114', 'home5', 'home89', 'home17', 'home63', 'home2', 'home101', 'home14', 'home95', 'home111', 'home67', 'home77', 'home43', 'home86', 'home90', 'home47', 'home110', 'home93', 'home53', 'home34', 'home51', 'home106', 'home46', 'home79', 'home55', 'home65', 'home13']\n","Number of different houses:\n","28\n"]}],"source":["# Different houses in data\n","houses = random_order_houses['house_id'].unique().tolist()\n","print(\"Different houses in data:\")\n","print(houses)\n","print(\"Number of different houses:\")\n","print(len(houses))"]},{"cell_type":"markdown","metadata":{"id":"fwbpJ2Qmj3N1"},"source":["**Δημιουργία του encoder decoder transformer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQypWHIuj3N2"},"outputs":[],"source":["# positional encoding layer\n","def positional_encoding(length, depth):\n","  depth = depth/2\n","  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","  angle_rates = 1 / (10000**depths)         # (1, depth)\n","  angle_rads = positions * angle_rates      # (pos, depth)\n","\n","  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","# positional embedding layer\n","class PositionalEmbedding(tf.keras.layers.Layer):\n","  #def __init__(self, vocab_size, d_model):\n","  def __init__(self, d_model):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.embedding = tf.keras.layers.Dense(d_model)  # Project input to d_model dimension\n","    #self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n","    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n","\n","  #def compute_mask(self, *args, **kwargs):\n","  #  return self.embedding.compute_mask(*args, **kwargs)\n","\n","  def call(self, x):\n","    # Remove the last dimension if it has size 1\n","    if tf.shape(x)[-1] == 1:\n","        x = tf.squeeze(x, axis=-1) # Maybe it helps. Remove the extra dimension (e.g., from (None, 1440, 1) to (None, 1440))\n","\n","    length = tf.shape(x)[1]\n","    x = self.embedding(x)\n","    # This factor sets the relative scale of the embedding and positonal_encoding.\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x = x + self.pos_encoding[tf.newaxis, :length, :]\n","    return x\n","\n","\n","\n","# Attention layers\n","# These are all identical except for how the attention is configured\n","\n","# base attention layer\n","class BaseAttention(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","    self.add = tf.keras.layers.Add()\n","\n","# cross attention layer\n","# (at the center of the Transformer is the cross-attention layer, it connects the encoder and decoder)\n","class CrossAttention(BaseAttention):\n","  def call(self, x, context):\n","    attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n","    # Cache the attention scores for plotting later.\n","    self.last_attn_scores = attn_scores\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","# global self attention layer\n","# (it is responsible for processing the context sequence, and propagating information along its length)\n","class GlobalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","# causal self attention layer\n","# (it does a similar job as the global self attention layer, for the output sequence)\n","class CausalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","    return x\n","\n","\n","\n","# feed forward network\n","# (the transformer also includes this point-wise feed-forward network in both the encoder and decoder)\n","class FeedForward(tf.keras.layers.Layer):\n","  def __init__(self, d_model, dff, dropout_rate=0.1):\n","    super().__init__()\n","    self.seq = tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),\n","      tf.keras.layers.Dense(d_model),\n","      tf.keras.layers.Dropout(dropout_rate)\n","    ])\n","    self.add = tf.keras.layers.Add()\n","    self.layer_norm = tf.keras.layers.LayerNormalization()\n","\n","  def call(self, x):\n","    x = self.add([x, self.seq(x)])\n","    x = self.layer_norm(x)\n","    return x\n","\n","\n","\n","# encoder layer\n","# (the encoder contains a stack of N encoder layers. Where each EncoderLayer contains\n","#   a GlobalSelfAttention and FeedForward layer)\n","class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x):\n","    x = self.self_attention(x)\n","    x = self.ffn(x)\n","    return x\n","\n","\n","\n","# The Encoder\n","class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    #self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","    self.pos_embedding = PositionalEmbedding(d_model=d_model)\n","    self.enc_layers = [\n","        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","  def call(self, x):\n","    # `x` is token-IDs shape: (batch, seq_len)\n","    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    # Add dropout.\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x)\n","    return x  # Shape `(batch_size, seq_len, d_model)`.\n","\n","\n","\n","# decoder layer\n","# (the decoder's stack is slightly more complex, with each DecoderLayer containing\n","#   a CausalSelfAttention, a CrossAttention, and a FeedForward layer)\n","class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","    self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x, context):\n","    x = self.causal_self_attention(x=x)\n","    x = self.cross_attention(x=x, context=context)\n","    # Cache the last attention scores for plotting later\n","    self.last_attn_scores = self.cross_attention.last_attn_scores\n","    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n","    return x\n","\n","\n","\n","# The Decoder\n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    #self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","    self.pos_embedding = PositionalEmbedding(d_model=d_model)\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","    self.dec_layers = [\n","        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","    self.last_attn_scores = None\n","\n","  def call(self, x, context):\n","    # `x` is token-IDs shape (batch, target_seq_len)\n","    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x = self.dropout(x)\n","    for i in range(self.num_layers):\n","      x  = self.dec_layers[i](x, context)\n","    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n","    # The shape of x is (batch_size, target_seq_len, d_model).\n","    return x\n","\n","\n","\n","# The Transformer\n","@tf.keras.utils.register_keras_serializable()\n","class Transformer(tf.keras.Model):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff,\n","               input_vocab_size, target_vocab_size, dropout_rate=0.1, **kwargs):\n","    super().__init__(**kwargs)\n","\n","    self.input_proj = tf.keras.layers.Dense(d_model)  # Project input to the model dimension\n","    self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n","    self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                           vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n","    #self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    self.final_layer = tf.keras.layers.Dense(1)\n","\n","  #def call(self, inputs, decoder_input=None):  # Expect separate decoder input\n","  #  x = inputs\n","  #  context = self.encoder(x)\n","  #  # If no decoder input is provided, assume we're in inference mode\n","  #  if decoder_input is None:\n","  #      # Initialize decoder input with zeros (batch_size, seq_len, d_model)\n","  #      decoder_input = tf.zeros_like(inputs)\n","  #  x = self.decoder(decoder_input, context)\n","  #  logits = self.final_layer(x)\n","  #  try:\n","  #      del logits._keras_mask\n","  #  except AttributeError:\n","  #      pass\n","  #  return logits\n","\n","  def call(self, inputs, decoder_input=None, training=False):\n","    # Encode the input sequence\n","    context = self.encoder(inputs)\n","    if training:\n","        # In training mode, we use the provided decoder input\n","        decoder_output = self.decoder(decoder_input, context)\n","    else:\n","        # In inference mode, initialize decoder input as a zero tensor or SOS token\n","        batch_size = tf.shape(inputs)[0]\n","        decoder_input = tf.zeros((batch_size, 1, self.d_model))  # Optionally, use an SOS embedding\n","        # For auto-regressive generation over 1440 minutes\n","        decoder_output = []\n","        for t in range(1440):  # or however many time steps you want to predict\n","            pred = self.decoder(decoder_input, context)\n","            output = self.final_layer(pred)  # Get predictions for this step\n","            decoder_output.append(output)\n","            # Update decoder input with the predicted output\n","            decoder_input = output\n","        # Concatenate outputs to match the desired shape\n","        decoder_output = tf.concat(decoder_output, axis=1)\n","    logits = self.final_layer(decoder_output)\n","    # Clear any masking that might interfere with training\n","    try:\n","        del logits._keras_mask\n","    except AttributeError:\n","        pass\n","\n","    return logits\n"]},{"cell_type":"markdown","metadata":{"id":"nt8LA8qyj3N3"},"source":["**Preparing the data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byevh-BWj3N3"},"outputs":[],"source":["# the categories for prediction\n","final_category = 'blr_mod_lvl'\n","prediction_categories = ['blr_mod_lvl', 'absorption', 'insulation', 't_r_set', 't_out']\n","normalized_categories = ['normalized_blr_mod_lvl', 'normalized_absorption', 'normalized_insulation', 'normalized_t_r_set', 'normalized_t_out']"]},{"cell_type":"code","source":["# creating sub-lists, each with the data of one day\n","# the function that gets category data of a house (ordered by date) and separates by day\n","def separate_into_days(data_list, minutes_per_day=1440):\n","    # number of days\n","    num_days = len(data_list) // minutes_per_day\n","    # the data into a list of sub-lists, each containing one day's data\n","    separated_data = [\n","        data_list[i * minutes_per_day:(i + 1) * minutes_per_day]\n","        for i in range(num_days)\n","    ]\n","    return separated_data\n","\n","\n","# function to \"combine\" values of categories and separate into sub-lists based on days\n","def combine_categories(dataset, categories_list):\n","    # Combine specified categories into lists\n","    combined_elements = dataset[categories_list].apply(lambda row: row.tolist(), axis=1)\n","    return combined_elements.tolist()"],"metadata":{"id":"8KH4RCDDgjIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_data_2(house_data, input_categories, output_category, minutes_per_day=1440):\n","    combined_input_data = combine_categories(house_data, input_categories)\n","    separated_input_data = separate_into_days(combined_input_data, minutes_per_day)\n","    output = house_data[output_category].values\n","    separated_output = separate_into_days(output, minutes_per_day)\n","\n","    input_data = separated_input_data[:-1]  # All except the last day for encoder input\n","    target_data = separated_output[1:]      # All except the first day for target output\n","    # For decoder input, shift target by one step\n","    decoder_input_data = separated_output[:-1]  # The previous day's output as decoder input\n","\n","    return input_data, decoder_input_data, target_data"],"metadata":{"id":"yg0AR2uqapGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gv-sbnlwj3N3"},"outputs":[],"source":["# function for getting input and target data\n","def input_target_split (data, input_categories, selected_houses):\n","    filtered_data = data[['house_id', 'time', 'normalized_blr_mod_lvl', 'normalized_absorption', 'normalized_insulation', 'normalized_t_r_set', 'normalized_t_out']].copy()\n","    input_chosen_categories = []\n","    for cat in input_categories:\n","        input_chosen_categories.append('normalized_'+cat)\n","\n","    # We have 1440 minutes per day\n","    minutes_per_day = 1440\n","\n","    # Initialize lists to store input and target pairs\n","    input_data_list, target_data_list, decoder_input_list = [], [], []\n","\n","    for house_id in selected_houses:\n","        house_data = filtered_data[filtered_data['house_id'] == house_id]\n","        house_data = house_data.sort_values(by='time')\n","        input_data, decoder_input_data, target_data = prepare_data_2(house_data, input_chosen_categories, 'normalized_blr_mod_lvl')\n","        input_data_list.append(input_data)\n","        decoder_input_list.append(decoder_input_data)\n","        target_data_list.append(target_data)\n","\n","    # Combine all houses' data\n","    input_data = np.concatenate(input_data_list, axis=0)\n","    decoder_input_data = np.concatenate(decoder_input_list, axis=0)\n","    target_data = np.concatenate(target_data_list, axis=0)\n","\n","    return input_data, decoder_input_data, target_data\n"]},{"cell_type":"code","source":["def get_split_data(train_data, val_data, input_categories, selected_houses):\n","    input_train, decoder_input_train, target_train = input_target_split(train_data, input_categories, selected_houses)\n","    input_val, decoder_input_val, target_val = input_target_split(val_data, input_categories, selected_houses)\n","    #input_test, decoder_input_test, target_test = input_target_split(test_data, input_categories, selected_houses)\n","    return input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val"],"metadata":{"id":"4ujpGxLKU9R9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=64"],"metadata":{"id":"m6SLLa-zhX6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_datasets(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val, batch_size=batch_size):\n","    # Reduce the batch size, in case it helps !\n","    # Convert to TensorFlow datasets\n","    train_dataset = tf.data.Dataset.from_tensor_slices(((input_train, decoder_input_train), target_train))\n","    #train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    train_dataset = train_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","    val_dataset = tf.data.Dataset.from_tensor_slices(((input_val, decoder_input_val), target_val))\n","    val_dataset = val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","    # !!! we do this for test_data separatelly later\n","\n","    # The validation_split argument is designed to work with NumPy arrays or TensorFlow tensors,\n","    # where it can easily split the data based on a fraction.\n","    # It doesn't directly work with TensorFlow datasets because they handle data differently.\n","    return train_dataset, val_dataset"],"metadata":{"id":"ou2SIOVlg1Bn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_shapes(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val):\n","    print(\"For train data: input, decoder_input, target  shapes are:\")\n","    print(input_train.shape)\n","    print(decoder_input_train.shape)\n","    print(target_train.shape)\n","    print(\"For val data: input, decoder_input, target  shapes are:\")\n","    print(input_val.shape)\n","    print(decoder_input_val.shape)\n","    print(target_val.shape)\n","    num_of_categories = input_train.shape[-1]\n","    print(\"Number of categories for prediction: \"+str(num_of_categories))\n","    return num_of_categories"],"metadata":{"id":"eHz6ftueyr1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to calculate the R2 during training and predicting\n","def r2_score(y_true, y_pred):\n","    # Flatten the arrays to compute R2 on all predicted values at once\n","    y_true_flat = tf.reshape(y_true, shape=(-1,))\n","    y_pred_flat = tf.reshape(y_pred, shape=(-1,))\n","\n","    # Ensure both tensors are of the same type (float32 in this case)\n","    y_true_flat = tf.cast(y_true_flat, tf.float32)\n","    y_pred_flat = tf.cast(y_pred_flat, tf.float32)\n","\n","    # Calculate R2\n","    ss_res = tf.reduce_sum(tf.square(y_true_flat - y_pred_flat))\n","    ss_tot = tf.reduce_sum(tf.square(y_true_flat - tf.reduce_mean(y_true_flat)))\n","\n","    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon()) # add small epsilon constant to avoid diivision by 0"],"metadata":{"id":"P7gaVhqy7uyC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeNznSxDx0kY"},"outputs":[],"source":["def make_transformer_model():\n","    minutes_per_day = 1440\n","    # Define the Transformer model\n","    #model = Transformer(num_layers=4, d_model=128, num_heads=8, dff=512, input_vocab_size=minutes_per_day,\n","    #                    target_vocab_size=minutes_per_day, dropout_rate=0.1) # minutes_per_day = 1440\n","    model = Transformer(num_layers=4, d_model=64, num_heads=4, dff=256, input_vocab_size=minutes_per_day,\n","                        target_vocab_size=minutes_per_day, dropout_rate=0.1) # minutes_per_day = 1440\n","    # Compile the model\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","                  loss=tf.keras.losses.MeanSquaredError(),\n","                  metrics=[tf.keras.metrics.MeanAbsoluteError(), r2_score])\n","    return model"]},{"cell_type":"code","source":["def train_transformer_model(model, train_dataset, val_dataset):\n","    # Define the EarlyStopping callback\n","    early_stopping = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',  # Metric to monitor\n","        patience=5,          # Number of epochs with no improvement after which training will be stopped\n","        verbose=1,           # Verbosity mode\n","        restore_best_weights=True  # Whether to restore model weights from the epoch with the best value of the monitored quantity\n","    )\n","    # Train the model with the EarlyStopping callback\n","    history = model.fit(\n","        train_dataset,       # Training dataset\n","        epochs=20,          # Number of epochs\n","        validation_data=val_dataset,  # Validation dataset\n","        callbacks=[early_stopping]  # List of callbacks to apply during training\n","    )\n","    # change epochs, patience , etc !!! (e.g. epochs=10, patience=3  or  epochs=20, patience=5)\n","    return model, history"],"metadata":{"id":"lzVc2U2WfVQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def training_plots(history):\n","    # Plotting the training and validation loss and MAE\n","    plt.figure(figsize=(14, 5))\n","\n","    # Plot training & validation loss values\n","    plt.subplot(2, 2, 1)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Model Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss (MSE)')\n","    plt.legend(loc='upper right')\n","\n","    # Plot training & validation MAE values\n","    plt.subplot(2, 2, 2)\n","    plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n","    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n","    plt.title('Model Mean Absolute Error')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('MAE')\n","    plt.legend(loc='upper right')\n","\n","    # Plot training & validation R2 values\n","    plt.subplot(2, 2, 3)\n","    plt.plot(history.history['r2_score'], label='Training R2')\n","    plt.plot(history.history['val_r2_score'], label='Validation R2')\n","    plt.title('Model R2 Score')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('R2 Score')\n","    plt.legend(loc='upper right')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"EIxdGKklzwG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_transformer_model(model, history, input_categories, num_of_houses, save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/models/'):\n","    # save the model\n","    # !!! I need to save the scalers and the input_test, target_test, test_dataset as well !!!\n","    # !!! Or I need to use the same for all models (save once and use for all) !!!\n","    categories_used = '_'.join(input_categories)\n","    houses_used = str(num_of_houses)\n","\n","    model_name = 'transformer_'+categories_used+\"_houses_\"+houses_used\n","\n","    # save trained model\n","    model.save(save_path + model_name + '.keras')\n","\n","    # save history of training\n","    with open(save_path + model_name + '_history.pkl', 'wb') as f:\n","      pickle.dump(history.history, f)"],"metadata":{"id":"sBGuQ4wCWr0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Load the scalers_dict\n","with open('/content/drive/MyDrive/boilers_drive/scalers.pkl', 'rb') as file:\n","    scalers_dict = pickle.load(file)"],"metadata":{"id":"j2fl98e_8DJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_test_values(test_values, house, category='blr_mod_lvl'):\n","    flattened_test_values = test_values.reshape(-1)\n","    if category=='blr_mod_lvl':\n","      de_scaled_test_values = (flattened_test_values.reshape(-1, 1)) * 100\n","      # de-scaling is multiplying by 100 for blr_mod_lvl values\n","      final_test_values = de_scaled_test_values.reshape(-1)\n","      #final_test_values = list(map(int, de_scaled_test_values))\n","    else:\n","      scaler = scalers_dict[house][category]\n","      test_values_reshaped = test_values.reshape(-1, 1)\n","      de_scaled_values = scaler.inverse_transform(test_values_reshaped)\n","      final_test_values = de_scaled_values.flatten()  # flatten de-scaled values\n","    return final_test_values\n","\n","# get predictions in list form per house\n","#def get_predictions(model, test_dataset, house, category='blr_mod_lvl'):\n","#    test_predictions = model.predict(test_dataset)\n","#    #print(\"predictions shape: \"+str(test_predictions.shape))\n","#    prediction = get_test_values(test_predictions, house, category)\n","#    #print(\"after processing, prediction shape: \"+str(prediction.shape))\n","#    return prediction\n","\n","def get_predictions(model, input_test, decoder_input_test, house, category='blr_mod_lvl'):\n","    # Predict using encoder and decoder inputs\n","    test_predictions = model.predict((input_test, decoder_input_test), training=False)\n","    # De-scale and flatten the predictions\n","    prediction = get_test_values(test_predictions, house, category)\n","    return prediction"],"metadata":{"id":"kuYVz2ANcY9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function that gives us the error metrics for given pair: target, prediction\n","def get_error_metrics(target, prediction):\n","    # error values\n","    error = [t - p for t, p in zip(target, prediction)]\n","    AE = [abs(e) for e in error] # Absolute Error\n","    SE = [e ** 2 for e in error] # Squared Error\n","\n","    # error metrics\n","    # Mean Absolute Error, gives magnitude of errors without caring for direction\n","    MAE = np.mean(AE)\n","    # Mean Squared Error, gives higher weight for larger errors\n","    MSE = np.mean(SE)\n","    # Root Mean Squared Error,  it is in the same units as the target variable\n","    RMSE = np.sqrt(MSE)\n","    # R-Squared, statistical measure that represents the proportion of the variance for the target variable that's explained by the model\n","    # provides an indication of the goodness of fit\n","    mean_target = np.mean(target)\n","    diff = [t - mean_target for t in target]\n","    denominator = [d ** 2 for d in diff]\n","    R2 = 1 - (sum(SE) / sum(denominator))\n","\n","    return [MAE, MSE, RMSE, R2]"],"metadata":{"id":"OgLdST1bdJLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_predictions_to_csv(target_values, prediction_values, model_name, house, save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'):\n","    folder_name = model_name + '_predictions'\n","    folder_path = os.path.join(save_path, folder_name)\n","\n","    # Check if the folder exists, and create it if it doesn't\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","        #print(f\"Folder created: {folder_path}\")\n","    #else:\n","        #print(f\"Folder already exists: {folder_path}\")\n","\n","    # the dataframe with target and prediction values\n","    df = pd.DataFrame({\n","        'Target': target_values,\n","        'Prediction': prediction_values\n","    })\n","\n","    file_name = house+'predictions.csv'\n","    file_path = os.path.join(folder_path, file_name)\n","\n","    # Save as a CSV file\n","    df.to_csv(file_path, index=False)"],"metadata":{"id":"p4PfH3jP8RM1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get test_dataset (only)\n","def get_test_dataset(input_test, decoder_input_test, target_test, batch_size=batch_size):\n","    # Reduce the batch size, in case it helps !\n","    test_dataset = tf.data.Dataset.from_tensor_slices(((input_test, decoder_input_test), target_test))\n","    test_dataset = test_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    return test_dataset\n","\n","# get test input and target for every house (separately) for given input_categories\n","#def get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl'):\n","#    test_values_per_house = {}\n","#    test_dataset_per_house = {}\n","#    for house in houses:\n","#        input_test, decoder_input_test, target_test = input_target_split(test_data[test_data[\"house_id\"]==house], input_categories, [house])\n","#        #print(\"for house with id: \"+house)\n","#        #print(\"input_test shape: \"+str(input_test.shape))\n","#        #print(\"target_test shape: \"+str(target_test.shape))\n","#        test_values = get_test_values(target_test, house, category)\n","#        test_values_per_house[house] = test_values\n","#        test_dataset_per_house[house] = get_test_dataset(input_test, decoder_input_test, target_test, batch_size=batch_size)\n","#    return test_values_per_house, test_dataset_per_house\n","\n","\n","def get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl'):\n","    test_values_per_house = {}\n","    test_input_per_house = {}\n","    test_decoder_input_per_house = {}\n","    for house in houses:\n","        # Retrieve encoder input, decoder input, and target\n","        input_test, decoder_input_test, target_test = input_target_split(test_data[test_data[\"house_id\"] == house], input_categories, [house])\n","\n","        test_values = get_test_values(target_test, house, category)\n","        test_values_per_house[house] = test_values\n","        test_input_per_house[house] = input_test\n","        test_decoder_input_per_house[house] = decoder_input_test\n","    return test_values_per_house, test_input_per_house, test_decoder_input_per_house\n","\n","\n","# function which calculates the average metrics for some houses\n","def calculate_average_metrics(metrics_per_house, selected_houses):\n","    MAE_sum, MSE_sum, RMSE_sum, R2_sum = 0, 0, 0, 0\n","    n = len(selected_houses)\n","    for house in selected_houses:\n","        [MAE, MSE, RMSE, R2] = metrics_per_house[house]\n","        MAE_sum += MAE\n","        MSE_sum += MSE\n","        RMSE_sum += RMSE\n","        R2_sum += R2\n","\n","    MAE_avg = MAE_sum / n\n","    MSE_avg = MSE_sum / n\n","    RMSE_avg = RMSE_sum / n\n","    R2_avg = R2_sum / n\n","\n","    return [MAE_avg, MSE_avg, RMSE_avg, R2_avg]\n","\n","# get prediction metrics per house for a model\n","# (assume correct input_categories for model and datasets per house)\n","# it also gives the avg metrics for relevant houses (the houses with which the model was trained) and all houses\n","#def get_prediction_metrics_per_house(model, model_name, num_of_houses, test_values_per_house, test_dataset_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'):\n","#    prediction_per_house = {}\n","#    error_metrics_per_house = {}\n","#    for house in houses:\n","#        prediction_per_house[house] = get_predictions(model, test_dataset_per_house[house], house, category)\n","#        save_predictions_to_csv(test_values_per_house[house], prediction_per_house[house], model_name, house, save_path)\n","#        [MAE, MSE, RMSE, R2] = get_error_metrics(test_values_per_house[house], prediction_per_house[house])\n","#        error_metrics_per_house[house] = [MAE, MSE, RMSE, R2]\n","#    for special in [\"relevant_houses\", \"all_houses\"]:\n","#        if (special==\"relevant_houses\"):\n","#            selected_houses = houses[:num_of_houses]\n","#        else: # in this case we use all the houses\n","#            selected_houses = houses\n","#        error_metrics_per_house[special] = calculate_average_metrics(error_metrics_per_house, selected_houses)\n","#    return error_metrics_per_house\n","\n","def get_prediction_metrics_per_house(model, model_name, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'):\n","    prediction_per_house = {}\n","    error_metrics_per_house = {}\n","    for house in houses:\n","        prediction_per_house[house] = get_predictions(model, test_input_per_house[house], test_decoder_input_per_house[house], house, category)\n","        save_predictions_to_csv(test_values_per_house[house], prediction_per_house[house], model_name, house, save_path)\n","        [MAE, MSE, RMSE, R2] = get_error_metrics(test_values_per_house[house], prediction_per_house[house])\n","        error_metrics_per_house[house] = [MAE, MSE, RMSE, R2]\n","    for special in [\"relevant_houses\", \"all_houses\"]:\n","        if (special==\"relevant_houses\"):\n","            selected_houses = houses[:num_of_houses]\n","        else:\n","            selected_houses = houses\n","        error_metrics_per_house[special] = calculate_average_metrics(error_metrics_per_house, selected_houses)\n","    return error_metrics_per_house\n","\n","\n","# get prediction metrics per house for a model in dataframe form\n","def get_metrics_dataframe(model_name, error_metrics_per_house, categories_str, num_of_houses, num_of_categories):\n","    # along with the individual houses we also have average metrics for the model\n","    # we add first the \"relevant\" and \"all\" before the individual houses in the list\n","    used_houses = [\"relevant_houses\", \"all_houses\"]\n","    for house in houses:\n","        used_houses.append(house)\n","\n","    data = {}\n","    data['categories'] = [categories_str]\n","    data['num_of_categories'] = [num_of_categories]\n","    data['num_of_houses'] = [num_of_houses]\n","    # Constructing the column names and filling them with metrics\n","    for house in used_houses:\n","        metrics = error_metrics_per_house[house]\n","        data['MAE_'+house] = [metrics[0]]\n","        data['MSE_'+house] = [metrics[1]]\n","        data['RMSE_'+house] = [metrics[2]]\n","        data['R2_'+house] = [metrics[3]]\n","\n","    # Create the dataframe\n","    new_house_df = pd.DataFrame(data, index=[model_name])\n","    return new_house_df"],"metadata":{"id":"srfW81E2mkv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading the transformer error metrics (until now) from csv and adding the new house (as a new row)\n","def update_transformer_error_metrics(new_house_df, model_name, path_to_transformer_error_metrics='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'):\n","    # load the existing dataframe from csv file\n","    try:\n","        transformer_error_metrics = pd.read_csv(path_to_transformer_error_metrics, index_col=0)\n","    except FileNotFoundError:\n","        # if the file does not exist, creates a new dataframe\n","        print(\"File not found. Creating a new one.\")\n","        transformer_error_metrics = new_house_df # there is no file, so the 'new' is made the 'transformer_error_metrics' file by default\n","        # save the updated dataframe back to csv file\n","        transformer_error_metrics.to_csv(path_to_transformer_error_metrics)\n","        return transformer_error_metrics\n","\n","    # align columns between existing dataframe and the new dataframe and then concatenate\n","    new_house_df = new_house_df.reindex(columns=transformer_error_metrics.columns, fill_value=pd.NA) # missing columns will be filled with value: 'NaN'\n","    # check if the model is already in the existing dataframe\n","    if model_name in transformer_error_metrics.index:\n","        # update the existing row with new values\n","        transformer_error_metrics.loc[model_name] = new_house_df.iloc[0]\n","    else:\n","        # append the new row if the model does not exist\n","        transformer_error_metrics = pd.concat([transformer_error_metrics, new_house_df], axis=0)\n","\n","    # save the updated dataframe back to csv file\n","    transformer_error_metrics = transformer_error_metrics.sort_values(by=['num_of_categories', 'categories', 'num_of_houses'])\n","    transformer_error_metrics.to_csv(path_to_transformer_error_metrics)\n","\n","    return transformer_error_metrics"],"metadata":{"id":"lF50zqwjn0ym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_to_transformer_error_metrics = '/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'\n","transformer_error_metrics = pd.read_csv(path_to_transformer_error_metrics, index_col=0)\n","transformer_error_metrics = transformer_error_metrics.sort_values(by=['num_of_categories', 'categories', 'num_of_houses'])\n","print(transformer_error_metrics.to_string(index=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"lE0NYSzhfTsZ","executionInfo":{"status":"error","timestamp":1730887618678,"user_tz":-120,"elapsed":1488,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"f7141ef6-7557-4446-8383-8d506600e631"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-5869ec5cb1af>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_to_transformer_error_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformer_error_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_transformer_error_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtransformer_error_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_error_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_of_categories'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'categories'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_of_houses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_error_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'"]}]},{"cell_type":"code","source":["# do it all completely\n","def transformer_model_complete(train_data, val_data, input_categories, num_of_houses, batch_size):\n","    print(\"splitting data\")\n","    selected_houses = houses[:num_of_houses]\n","    input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val = get_split_data(train_data, val_data, input_categories, selected_houses)\n","    print(\"getting datasets\")\n","    train_dataset, val_dataset = get_datasets(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val, batch_size=batch_size)\n","    print(\"printing shapes and getting num_of_categories\")\n","    num_of_categories = print_shapes(input_train, decoder_input_train, target_train, input_val, decoder_input_val, target_val)\n","    print(\"making transformer model\")\n","    model = make_transformer_model()\n","    print(\"training transformer model\")\n","    model, history = train_transformer_model(model, train_dataset, val_dataset)\n","    print(\"the training plots\")\n","    training_plots(history)\n","    #print(\"saving the model and the training history\")\n","    #save_transformer_model(model, history, input_categories, num_of_houses)\n","    return model, history, input_categories, num_of_houses\n","\n","def doing_testing(model, input_categories, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/', path_to_transformer_error_metrics='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'):\n","    num_of_categories = len(input_categories)\n","    categories_used = '_'.join(input_categories)\n","    houses_used = str(num_of_houses)\n","    model_name = 'transformer_'+categories_used+\"_houses_\"+houses_used\n","    print(\"getting error metrics\")\n","    error_metrics_per_house = get_prediction_metrics_per_house(model, model_name, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category, save_path)\n","    new_house_df = get_metrics_dataframe(model_name, error_metrics_per_house, categories_used, num_of_houses, num_of_categories)\n","    print(\"updating the transformer error metrics\")\n","    transformer_error_metrics = update_transformer_error_metrics(new_house_df, model_name, path_to_transformer_error_metrics) # path for load/store is already given\n","    print(\"the new transformer error metrics file:\")\n","    print(transformer_error_metrics.to_string(index=True))\n","    print(\"--------------------\")\n","    print(\"finished\")\n"],"metadata":{"id":"uxkp7TNYXHTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prediction_categories)\n","\n","input_categories = [prediction_categories[0]]\n","\n","num_of_houses = 1\n","#num_of_houses = 7\n","#num_of_houses = len(houses)\n","\n","possible_num_of_houses = [1, 4, len(houses)] # 1 house, 4 houses (which is about 1/8 of all 28 houses), all houses (28 houses)\n","\n","print(input_categories)\n","print(num_of_houses)\n","print(possible_num_of_houses)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJHKUtGNXpGu","executionInfo":{"status":"ok","timestamp":1730887949807,"user_tz":-120,"elapsed":288,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"cc3ba743-e3b9-4c48-a6ad-7a622d70ce08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['blr_mod_lvl', 'absorption', 'insulation', 't_r_set', 't_out']\n","['blr_mod_lvl']\n","1\n","[1, 4, 28]\n"]}]},{"cell_type":"code","source":["print(prediction_categories[:1])\n","print(possible_num_of_houses[:1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9U7smFY7rPqR","executionInfo":{"status":"ok","timestamp":1730887951008,"user_tz":-120,"elapsed":286,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}},"outputId":"0f6afdc8-2511-4c7b-de37-fe481e7a3022"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['blr_mod_lvl']\n","[1]\n"]}]},{"cell_type":"code","source":["# !!! all models have been trained and saved (problems with loading saved models) !!!\n","# !!! geting error_metrics !!!\n","batch_size=8\n","#batch_size=16 # it is a bit slow, for greater batch_sizes there is error\n","\n","# ! Metrics are shown for the last batch\n","\n","for input_category in prediction_categories[:1]:\n","    input_categories = [input_category]\n","    #test_values_per_house, test_dataset_per_house = get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl')\n","    test_values_per_house, test_input_per_house, test_decoder_input_per_house = get_test_data_per_house(input_categories, batch_size=batch_size, category='blr_mod_lvl')\n","    for num_of_houses in possible_num_of_houses[:1]:\n","        print(\"---NEW Transformer MODEL---\")\n","        print(\"input_categories: \"+str(input_categories))\n","        print(\"num_of_houses: \"+str(num_of_houses))\n","        print(\"--------------------\")\n","        model, history, input_categories, num_of_houses = transformer_model_complete(train_data, val_data, input_categories, num_of_houses, batch_size=batch_size)\n","        doing_testing(model, input_categories, num_of_houses, test_values_per_house, test_input_per_house, test_decoder_input_per_house, category='blr_mod_lvl', save_path='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/', path_to_transformer_error_metrics='/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv')\n","        print(\"--------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mJ5-8TyaX3w-","outputId":"32b1c283-6f14-4552-d148-dd417c0ee898","executionInfo":{"status":"error","timestamp":1730889249515,"user_tz":-120,"elapsed":15232,"user":{"displayName":"ΓΕΩΡΓΙΟΣ ΚΑΛΑΡΑΣ","userId":"15058443889329994423"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---NEW Transformer MODEL---\n","input_categories: ['blr_mod_lvl']\n","num_of_houses: 1\n","--------------------\n","splitting data\n","getting datasets\n","printing shapes and getting num_of_categories\n","For train data: input, decoder_input, target  shapes are:\n","(134, 1440, 1)\n","(134, 1440)\n","(134, 1440)\n","For val data: input, decoder_input, target  shapes are:\n","(33, 1440, 1)\n","(33, 1440)\n","(33, 1440)\n","Number of categories for prediction: 1\n","making transformer model\n","training transformer model\n","Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1331: UserWarning: Layer 'positional_embedding_6' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n","1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n","2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n","Exception encountered: ''Shapes must be equal rank, but are 3 and 2\n","\tFrom merging shape 0 with other shapes. for '{{node Shape/packed}} = Pack[N=2, T=DT_FLOAT, axis=0](Shape/packed/Cast, Shape/packed/Cast_1)' with input shapes: [?,1440,1], [?,1440].''\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'positional_embedding_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1331: UserWarning: Layer 'encoder_4' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n","1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n","2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n","Exception encountered: ''Exception encountered when calling PositionalEmbedding.call().\n","\n","\u001b[1mShapes must be equal rank, but are 3 and 2\n","\tFrom merging shape 0 with other shapes. for '{{node positional_embedding_6_1/Shape/packed}} = Pack[N=2, T=DT_FLOAT, axis=0](positional_embedding_6_1/Shape/packed/Cast, positional_embedding_6_1/Shape/packed/Cast_1)' with input shapes: [?,1440,1], [?,1440].\u001b[0m\n","\n","Arguments received by PositionalEmbedding.call():\n","  • x=('tf.Tensor(shape=(None, 1440, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1440), dtype=float32)')''\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'encoder_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1331: UserWarning: Layer 'transformer_4' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n","1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n","2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n","Exception encountered: ''Exception encountered when calling PositionalEmbedding.call().\n","\n","\u001b[1mShapes must be equal rank, but are 3 and 2\n","\tFrom merging shape 0 with other shapes. for '{{node encoder_4_1/positional_embedding_6_1/Shape/packed}} = Pack[N=2, T=DT_FLOAT, axis=0](encoder_4_1/positional_embedding_6_1/Shape/packed/Cast, encoder_4_1/positional_embedding_6_1/Shape/packed/Cast_1)' with input shapes: [?,1440,1], [?,1440].\u001b[0m\n","\n","Arguments received by PositionalEmbedding.call():\n","  • x=('tf.Tensor(shape=(None, 1440, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1440), dtype=float32)')''\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'transformer_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"ValueError","evalue":"Exception encountered when calling PositionalEmbedding.call().\n\n\u001b[1mShapes must be equal rank, but are 3 and 2\n\tFrom merging shape 0 with other shapes. for '{{node transformer_4_1/encoder_4_1/positional_embedding_6_1/Shape/packed}} = Pack[N=2, T=DT_FLOAT, axis=0](Cast, Cast_1)' with input shapes: [?,1440,1], [?,1440].\u001b[0m\n\nArguments received by PositionalEmbedding.call():\n  • x=('tf.Tensor(shape=(None, 1440, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1440), dtype=float32)')","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-6e2bbd9fd975>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_of_houses: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_houses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_houses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_model_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_houses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdoing_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_houses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_values_per_house\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_per_house\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_decoder_input_per_house\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blr_mod_lvl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_predictions/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_transformer_error_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-1e7803f8e847>\u001b[0m in \u001b[0;36mtransformer_model_complete\u001b[0;34m(train_data, val_data, input_categories, num_of_houses, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training transformer model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the training plots\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtraining_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-c59fd515f6ae>\u001b[0m in \u001b[0;36mtrain_transformer_model\u001b[0;34m(model, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Train the model with the EarlyStopping callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# Training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-ebaa740795fd>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, decoder_input, training)\u001b[0m\n\u001b[1;32m    222\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# Encode the input sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# In training mode, we use the provided decoder input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-ebaa740795fd>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# `x` is token-IDs shape: (batch, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape `(batch_size, seq_len, d_model)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;31m# Add dropout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-ebaa740795fd>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Remove the last dimension if it has size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Maybe it helps. Remove the extra dimension (e.g., from (None, 1440, 1) to (None, 1440))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling PositionalEmbedding.call().\n\n\u001b[1mShapes must be equal rank, but are 3 and 2\n\tFrom merging shape 0 with other shapes. for '{{node transformer_4_1/encoder_4_1/positional_embedding_6_1/Shape/packed}} = Pack[N=2, T=DT_FLOAT, axis=0](Cast, Cast_1)' with input shapes: [?,1440,1], [?,1440].\u001b[0m\n\nArguments received by PositionalEmbedding.call():\n  • x=('tf.Tensor(shape=(None, 1440, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1440), dtype=float32)')"]}]},{"cell_type":"code","source":["path_to_transformer_error_metrics = '/content/drive/Othercomputers/My_Laptop/code/updated_transformers/transformer_error_metrics.csv'\n","transformer_error_metrics = pd.read_csv(path_to_transformer_error_metrics, index_col=0)\n","transformer_error_metrics = transformer_error_metrics.sort_values(by=['num_of_houses', 'categories', 'num_of_categories'])\n","print(transformer_error_metrics.to_string(index=True))"],"metadata":{"id":"Yl4wENP7Deld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**------------------------------------------------------------------**\n","\n","---\n","**-------------------Μερικές Δοκιμές---------------------------**\n","\n","---\n","**-------------------------------------------------------------------**"],"metadata":{"id":"9Dg5VHpypiBE"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}