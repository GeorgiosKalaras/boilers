{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data:\n",
      "                        time    blr_mod_lvl  absorption  insulation  t_r_set  \\\n",
      "0        2022-10-01 00:00:00   0.000000e+00    0.503910   -7.457292     15.0   \n",
      "1        2022-10-01 00:01:00   0.000000e+00    0.503910   -7.455208     15.0   \n",
      "2        2022-10-01 00:02:00   0.000000e+00    0.518558   -7.487500     15.0   \n",
      "3        2022-10-01 00:03:00   0.000000e+00    0.616207   -7.426042     15.0   \n",
      "4        2022-10-01 00:04:00   0.000000e+00    0.699210   -7.425000     15.0   \n",
      "...                      ...            ...         ...         ...      ...   \n",
      "7331983  2023-04-30 23:55:00  2.871866e-119   -0.296880   -1.604167     17.0   \n",
      "7331984  2023-04-30 23:56:00  1.914578e-119   -0.296880   -1.614583     17.0   \n",
      "7331985  2023-04-30 23:57:00  1.276385e-119   -0.296880   -1.572917     17.0   \n",
      "7331986  2023-04-30 23:58:00  8.509234e-120   -0.296880   -1.511458     17.0   \n",
      "7331987  2023-04-30 23:59:00  5.672823e-120   -0.296880   -1.432203     17.0   \n",
      "\n",
      "             t_out house_id      blr_t  heat  flame  ...      t_ret   t_r  \\\n",
      "0        22.342708    home2  28.398440   0.0    0.0  ...  27.894530  29.8   \n",
      "1        22.344792    home2  28.398440   0.0    0.0  ...  27.894530  29.8   \n",
      "2        22.312500    home2  28.413088   0.0    0.0  ...  27.894530  29.8   \n",
      "3        22.373958    home2  28.479815   0.0    0.0  ...  27.863608  29.8   \n",
      "4        22.375000    home2  28.496090   0.0    0.0  ...  27.796880  29.8   \n",
      "...            ...      ...        ...   ...    ...  ...        ...   ...   \n",
      "7331983  17.395833  home114  20.000000   0.0    0.0  ...  20.296880  19.0   \n",
      "7331984  17.385417  home114  20.000000   0.0    0.0  ...  20.296880  19.0   \n",
      "7331985  17.427083  home114  20.000000   0.0    0.0  ...  20.296880  19.0   \n",
      "7331986  17.488542  home114  20.000000   0.0    0.0  ...  20.296880  19.0   \n",
      "7331987  17.567797  home114  20.000000   0.0    0.0  ...  20.296880  19.0   \n",
      "\n",
      "         otc_cur  t_set   otc_maxt  bypass  nodata  month  day  hour  \n",
      "0            3.0    0.0  45.253125     1.0     0.0     10    1     0  \n",
      "1            3.0    0.0  45.245562     1.0     0.0     10    1     0  \n",
      "2            3.0    0.0  45.273750     1.0     0.0     10    1     0  \n",
      "3            3.0    0.0  45.240063     1.0     0.0     10    1     0  \n",
      "4            3.0    0.0  45.232500     1.0     0.0     10    1     0  \n",
      "...          ...    ...        ...     ...     ...    ...  ...   ...  \n",
      "7331983      3.0    0.0  48.525625     1.0     0.0      4   30    23  \n",
      "7331984      3.0    0.0  48.518750     1.0     0.0      4   30    23  \n",
      "7331985      3.0    0.0  48.505000     1.0     0.0      4   30    23  \n",
      "7331986      3.0    0.0  48.464438     1.0     0.0      4   30    23  \n",
      "7331987      3.0    0.0  48.419237     1.0     0.0      4   30    23  \n",
      "\n",
      "[7331988 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path of your CSV file\n",
    "file_path = 'normalized_df.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Imported data:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different houses in data:\n",
      "['home2' 'home9' 'home13' 'home14' 'home34' 'home46' 'home55' 'home67'\n",
      " 'home86' 'home93' 'home101' 'home106' 'home110' 'home43' 'home63'\n",
      " 'home53' 'home79' 'home90' 'home95' 'home5' 'home17' 'home47' 'home51'\n",
      " 'home65' 'home77' 'home89' 'home111' 'home114']\n",
      "Number of different houses:\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# Different houses in data\n",
    "houses = data['house_id'].unique()\n",
    "print(\"Different houses in data:\")\n",
    "print(houses)\n",
    "print(\"Number of different houses:\")\n",
    "print(len(houses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_datasets = {}\n",
    "for house in houses:\n",
    "    house_datasets[house] = data[data['house_id'] == house]\n",
    "\n",
    "#print(house_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Δημιουργία του encoder decoder transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding layer\n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# positional embedding layer\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention layers\n",
    "# These are all identical except for how the attention is configured\n",
    "\n",
    "# base attention layer\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "# cross attention layer\n",
    "# (at the center of the Transformer is the cross-attention layer, it connects the encoder and decoder)\n",
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# global self attention layer\n",
    "# (it is responsible for processing the context sequence, and propagating information along its length)\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# causal self attention layer\n",
    "# (it does a similar job as the global self attention layer, for the output sequence)\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward network\n",
    "# (the transformer also includes this point-wise feed-forward network in both the encoder and decoder)\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder layer\n",
    "# (the encoder contains a stack of N encoder layers. Where each EncoderLayer contains a GlobalSelfAttention and FeedForward layer)\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Encoder\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "      \n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder layer\n",
    "# (the decoder's stack is slightly more complex, with each DecoderLayer containing\n",
    "#   a CausalSelfAttention, a CrossAttention, and a FeedForward layer)\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x = self.dropout(x)\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "    \n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
